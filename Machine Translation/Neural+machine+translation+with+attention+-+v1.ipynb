{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation\n",
    "\n",
    "Welcome to your first programming assignment for this week! \n",
    "\n",
    "You will build a Neural Machine Translation (NMT) model to translate human readable dates (\"25th of June, 2009\") into machine readable dates (\"2009-06-25\"). You will do this using an attention model, one of the most sophisticated sequence to sequence models. \n",
    "\n",
    "This notebook was produced together with NVIDIA's Deep Learning Institute. \n",
    "\n",
    "Let's load all the packages you will need for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, CuDNNLSTM, Multiply\n",
    "from keras.layers import RepeatVector, Dense, Activation,Lambda,Softmax,Reshape\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from faker import Faker\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from babel.dates import format_date\n",
    "from nmt_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  - Translating human readable dates into machine readable dates\n",
    "\n",
    "The model you will build here could be used to translate from one language to another, such as translating from English to Hindi. However, language translation requires massive datasets and usually takes days of training on GPUs. To give you a place to experiment with these models even without using massive datasets, we will instead use a simpler \"date translation\" task. \n",
    "\n",
    "The network will input a date written in a variety of possible formats (*e.g. \"the 29th of August 1958\", \"03/30/1968\", \"24 JUNE 1987\"*) and translate them into standardized, machine readable dates (*e.g. \"1958-08-29\", \"1968-03-30\", \"1987-06-24\"*). We will have the network learn to output dates in the common machine-readable format YYYY-MM-DD. \n",
    "\n",
    "\n",
    "\n",
    "<!-- \n",
    "Take a look at [nmt_utils.py](./nmt_utils.py) to see all the formatting. Count and figure out how the formats work, you will need this knowledge later. !--> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  - Dataset\n",
    "\n",
    "We will train the model on a dataset of 10000 human readable dates and their equivalent, standardized, machine readable dates. Let's run the following cells to load the dataset and print some examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 25311.93it/s]\n"
     ]
    }
   ],
   "source": [
    "m = 10000\n",
    "dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('9 may 1998', '1998-05-09'),\n",
       " ('10.11.19', '2019-11-10'),\n",
       " ('9/10/70', '1970-09-10'),\n",
       " ('saturday april 28 1990', '1990-04-28'),\n",
       " ('thursday january 26 1995', '1995-01-26'),\n",
       " ('monday march 7 1983', '1983-03-07'),\n",
       " ('sunday may 22 1988', '1988-05-22'),\n",
       " ('08 jul 2008', '2008-07-08'),\n",
       " ('8 sep 1999', '1999-09-08'),\n",
       " ('thursday january 1 1981', '1981-01-01')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've loaded:\n",
    "- `dataset`: a list of tuples of (human readable date, machine readable date)\n",
    "- `human_vocab`: a python dictionary mapping all characters used in the human readable dates to an integer-valued index \n",
    "- `machine_vocab`: a python dictionary mapping all characters used in machine readable dates to an integer-valued index. These indices are not necessarily consistent with `human_vocab`. \n",
    "- `inv_machine_vocab`: the inverse dictionary of `machine_vocab`, mapping from indices back to characters. \n",
    "\n",
    "Let's preprocess the data and map the raw text data into the index values. We will also use Tx=30 (which we assume is the maximum length of the human readable date; if we get a longer input, we would have to truncate it) and Ty=10 (since \"YYYY-MM-DD\" is 10 characters long). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#human_vocab\n",
    "#machine_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (10000, 30)\n",
      "Y.shape: (10000, 10)\n",
      "Xoh.shape: (10000, 30, 37)\n",
      "Yoh.shape: (10000, 10, 11)\n"
     ]
    }
   ],
   "source": [
    "Tx = 30\n",
    "Ty = 10\n",
    "X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n",
    "\n",
    "print(\"X.shape:\", X.shape)\n",
    "print(\"Y.shape:\", Y.shape)\n",
    "print(\"Xoh.shape:\", Xoh.shape)\n",
    "print(\"Yoh.shape:\", Yoh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4,  7,  0, 29, 17, 27, 30, 17, 24, 14, 17, 28,  0,  5,  3,  3,  5,\n",
       "       36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0] #'<pad>': 36"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have:\n",
    "- `X`: a processed version of the human readable dates in the training set, where each character is replaced by an index mapped to the character via `human_vocab`. Each date is further padded to $T_x$ values with a special character (< pad >). `X.shape = (m, Tx)`\n",
    "- `Y`: a processed version of the machine readable dates in the training set, where each character is replaced by the index it is mapped to in `machine_vocab`. You should have `Y.shape = (m, Ty)`. \n",
    "- `Xoh`: one-hot version of `X`, the \"1\" entry's index is mapped to the character thanks to `human_vocab`. `Xoh.shape = (m, Tx, len(human_vocab))`\n",
    "- `Yoh`: one-hot version of `Y`, the \"1\" entry's index is mapped to the character thanks to `machine_vocab`. `Yoh.shape = (m, Tx, len(machine_vocab))`. Here, `len(machine_vocab) = 11` since there are 11 characters ('-' as well as 0-9). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets also look at some examples of preprocessed training examples. Feel free to play with `index` in the cell below to navigate the dataset and see how source/target dates are preprocessed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source date: 14 september 2002\n",
      "Target date: 2002-09-14\n",
      "\n",
      "Source after preprocessing (indices): 4\n",
      "Target after preprocessing (indices): 3\n",
      "\n",
      "Source after preprocessing (one-hot): [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Target after preprocessing (one-hot): [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "print(\"Source date:\", dataset[index][0])\n",
    "print(\"Target date:\", dataset[index][1])\n",
    "print()\n",
    "print(\"Source after preprocessing (indices):\", X[index][0])\n",
    "print(\"Target after preprocessing (indices):\", Y[index][0])\n",
    "print()\n",
    "print(\"Source after preprocessing (one-hot):\", Xoh[index][0])\n",
    "print(\"Target after preprocessing (one-hot):\", Yoh[index][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Neural machine translation with attention\n",
    "\n",
    "If you had to translate a book's paragraph from French to English, you would not read the whole paragraph, then close the book and translate. Even during the translation process, you would read/re-read and focus on the parts of the French paragraph corresponding to the parts of the English you are writing down. \n",
    "\n",
    "The attention mechanism tells a Neural Machine Translation model where it should pay attention to at any step. \n",
    "\n",
    "\n",
    "###  Attention mechanism\n",
    "\n",
    "In this part, you will implement the attention mechanism presented in the lecture videos. Here is a figure to remind you how the model works. The diagram on the left shows the attention model. The diagram on the right shows what one \"Attention\" step does to calculate the attention variables $\\alpha^{\\langle t, t' \\rangle}$, which are used to compute the context variable $context^{\\langle t \\rangle}$ for each timestep in the output ($t=1, \\ldots, T_y$). \n",
    "\n",
    "<table>\n",
    "<td> \n",
    "<img src=\"images/attn_model.png\" style=\"width:500;height:500px;\"> <br>\n",
    "</td> \n",
    "<td> \n",
    "<img src=\"images/attn_mechanism.png\" style=\"width:500;height:500px;\"> <br>\n",
    "</td> \n",
    "</table>\n",
    "<caption><center> **Figure 1**: Neural machine translation with attention</center></caption>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here are some properties of the model that you may notice: \n",
    "\n",
    "- There are two separate LSTMs in this model (see diagram on the left). Because the one at the bottom of the picture is a Bi-directional LSTM and comes *before* the attention mechanism, we will call it *pre-attention* Bi-LSTM. The LSTM at the top of the diagram comes *after* the attention mechanism, so we will call it the *post-attention* LSTM. The pre-attention Bi-LSTM goes through $T_x$ time steps; the post-attention LSTM goes through $T_y$ time steps. \n",
    "\n",
    "- The post-attention LSTM passes $s^{\\langle t \\rangle}, c^{\\langle t \\rangle}$ from one time step to the next. In the lecture videos, we were using only a basic RNN for the post-activation sequence model, so the state captured by the RNN output activations $s^{\\langle t\\rangle}$. But since we are using an LSTM here, the LSTM has both the output activation $s^{\\langle t\\rangle}$ and the hidden cell state $c^{\\langle t\\rangle}$. However, unlike previous text generation examples (such as Dinosaurus in week 1), in this model the post-activation LSTM at time $t$ does will not take the specific generated $y^{\\langle t-1 \\rangle}$ as input; it only takes $s^{\\langle t\\rangle}$ and $c^{\\langle t\\rangle}$ as input. We have designed the model this way, because (unlike language generation where adjacent characters are highly correlated) there isn't as strong a dependency between the previous character and the next character in a YYYY-MM-DD date. \n",
    "\n",
    "- We use $a^{\\langle t \\rangle} = [\\overrightarrow{a}^{\\langle t \\rangle}; \\overleftarrow{a}^{\\langle t \\rangle}]$ to represent the concatenation of the activations of both the forward-direction and backward-directions of the pre-attention Bi-LSTM. \n",
    "\n",
    "- The diagram on the right uses a `RepeatVector` node to copy $s^{\\langle t-1 \\rangle}$'s value $T_x$ times, and then `Concatenation` to concatenate $s^{\\langle t-1 \\rangle}$ and $a^{\\langle t \\rangle}$ to compute $e^{\\langle t, t'}$, which is then passed through a softmax to compute $\\alpha^{\\langle t, t' \\rangle}$. We'll explain how to use `RepeatVector` and `Concatenation` in Keras below. \n",
    "\n",
    "Lets implement this model. You will start by implementing two functions: `one_step_attention()` and `model()`.\n",
    "\n",
    "**1) `one_step_attention()`**: At step $t$, given all the hidden states of the Bi-LSTM ($[a^{<1>},a^{<2>}, ..., a^{<T_x>}]$) and the previous hidden state of the second LSTM ($s^{<t-1>}$), `one_step_attention()` will compute the attention weights ($[\\alpha^{<t,1>},\\alpha^{<t,2>}, ..., \\alpha^{<t,T_x>}]$) and output the context vector (see Figure  1 (right) for details):\n",
    "$$context^{<t>} = \\sum_{t' = 0}^{T_x} \\alpha^{<t,t'>}a^{<t'>}\\tag{1}$$ \n",
    "\n",
    "Note that we are denoting the attention in this notebook $context^{\\langle t \\rangle}$. In the lecture videos, the context was denoted $c^{\\langle t \\rangle}$, but here we are calling it $context^{\\langle t \\rangle}$ to avoid confusion with the (post-attention) LSTM's internal memory cell variable, which is sometimes also denoted $c^{\\langle t \\rangle}$. \n",
    "  \n",
    "**2) `model()`**: Implements the entire model. It first runs the input through a Bi-LSTM to get back $[a^{<1>},a^{<2>}, ..., a^{<T_x>}]$. Then, it calls `one_step_attention()` $T_y$ times (`for` loop). At each iteration of this loop, it gives the computed context vector $c^{<t>}$ to the second LSTM, and runs the output of the LSTM through a dense layer with softmax activation to generate a prediction $\\hat{y}^{<t>}$. \n",
    "\n",
    "\n",
    "\n",
    "**Exercise**: Implement `one_step_attention()`. The function `model()` will call the layers in `one_step_attention()` $T_y$ using a for-loop, and it is important that all $T_y$ copies have the same weights. I.e., it should not re-initiaiize the weights every time. In other words, all $T_y$ steps should have shared weights. Here's how you can implement layers with shareable weights in Keras:\n",
    "1. Define the layer objects (as global variables for examples).\n",
    "2. Call these objects when propagating the input.\n",
    "\n",
    "We have defined the layers you need as global variables. Please run the following cells to create them. Please check the Keras documentation to make sure you understand what these layers are: [RepeatVector()](https://keras.io/layers/core/#repeatvector), [Concatenate()](https://keras.io/layers/merge/#concatenate), [Dense()](https://keras.io/layers/core/#dense), [Activation()](https://keras.io/layers/core/#activation), [Dot()](https://keras.io/layers/merge/#dot)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use these layers to implement `one_step_attention()`. In order to propagate a Keras tensor object X through one of these layers, use `layer(X)` (or `layer([X,Y])` if it requires multiple inputs.), e.g. `densor(X)` will propagate X through the `Dense(1)` layer defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M1: 使用 全局的 层对象，以在多个 model 中共享他们的权重\n",
    "\n",
    "# GRADED FUNCTION: one_step_attention\n",
    "# Defined shared layers as global variables\n",
    "repeator = RepeatVector(Tx)  \n",
    "#concatenator = Concatenate(axis=-1)\n",
    "concatenator = Concatenate(axis=2)\n",
    "densor = Dense(1, activation = \"relu\")\n",
    "#activator = Activation(softmax, name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n",
    "activator=Softmax(axis=1)\n",
    "dotor = Dot(axes = 1)\n",
    "\n",
    "\n",
    "def one_step_attention(a, s_prev): #与RNN 类似，是一个 循环结构\n",
    "    \"\"\"\n",
    "    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n",
    "    \"alphas\" and the hidden states \"a\" of the Bi-LSTM.\n",
    "    \n",
    "    Arguments:\n",
    "    a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)\n",
    "    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n",
    "    \n",
    "    Returns:\n",
    "    context -- context vector, input of the next (post-attetion) LSTM cell\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Use repeator to repeat s_prev to be of shape (m, Tx, n_s) so that you can concatenate it with all hidden states \"a\" (≈ 1 line)\n",
    "    s_prev = repeator(s_prev) \n",
    "    \n",
    "    # Use concatenator to concatenate a and s_prev on the last axis (≈ 1 line) \n",
    "    concat =concatenator([a,s_prev]) #shape: (m, Tx, 2*n_a+n_s)\n",
    "                                      \n",
    "    # Use densor to propagate concat through a small fully-connected neural network to compute the \"energies\" variable e. (≈1 lines)\n",
    "    e = densor(concat) #  shape: (m, Tx, 1)\n",
    "                    \n",
    "    # Use activator and e to compute the attention weights \"alphas\" (≈ 1 line)\n",
    "    alphas = activator(e) # shape:  (m, Tx, 1)\n",
    "    \n",
    "    # Use dotor together with \"alphas\" and \"a\" to compute the context vector to be given to the next (post-attention) LSTM-cell (≈ 1 line)\n",
    "    context = dotor([alphas,a]) #  shape: (m, 1, 2*n_a)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By XRH in 2019.12.16\n",
    "# M2: 把层 layer 包装为 model ,并通过重新定义 model 的输入的方式 来共享 layer 的权重 \n",
    "\n",
    "n_a = 64\n",
    "n_s = 128 \n",
    "\n",
    "\n",
    "def one_step_attention_model(Tx, n_a, n_s): \n",
    "\n",
    "    \"\"\" \n",
    "    Arguments:\n",
    "    a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)\n",
    "    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n",
    "    \n",
    "    Returns:\n",
    "    context -- context vector, input of the next (post-attetion) LSTM cell\n",
    "    \"\"\"\n",
    "    \n",
    "    repeator = RepeatVector(Tx)  \n",
    "    concatenator = Concatenate(axis=2)\n",
    "    densor = Dense(1, activation = \"relu\")\n",
    "    activator=Softmax(axis=1)\n",
    "    dotor = Dot(axes = 1)\n",
    "    \n",
    "    \n",
    "    a0=Input(shape=(Tx, 2*n_a), name='a')\n",
    "    s_prev0=Input(shape=(n_s,), name='s_prev')\n",
    "    \n",
    "    a=a0 # 否则报错 ： ValueError: Graph disconnected: cannot obtain value for tensor Tensor .... The following previous layers were accessed without issue: []\n",
    "    s_prev=s_prev0\n",
    "    \n",
    "    s_prev = repeator(s_prev) \n",
    "    \n",
    "    concat =concatenator([a,s_prev]) #shape: (m, Tx, 2*n_a+n_s)\n",
    "                                      \n",
    "    e = densor(concat) #  shape: (m, Tx, 1)\n",
    "                    \n",
    "    alphas = activator(e) # shape:  (m, Tx, 1)\n",
    "    \n",
    "    context = dotor([alphas,a]) #  shape: (m, 1, 2*n_a)\n",
    "    \n",
    "    model=Model(inputs=[a0, s_prev0] ,outputs=context)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model_one_step_attention = one_step_attention_model(Tx, n_a, n_s)\n",
    "\n",
    "def one_step_attention_M2(a, s_prev): \n",
    "        \n",
    "    context=model_one_step_attention([a, s_prev])\n",
    "    \n",
    "    return context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will be able to check the expected output of `one_step_attention()` after you've coded the `model()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Implement `model()` as explained in figure 2 and the text above. Again, we have defined global layers that will share weights to be used in `model()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_a = 64\n",
    "n_s = 128\n",
    "\n",
    "# n_s = 64\n",
    "\n",
    "pre_activation_LSTM_cell=Bidirectional(CuDNNLSTM(n_a, return_sequences=True,return_state = True))\n",
    "\n",
    "post_activation_LSTM_cell = CuDNNLSTM(n_s, return_state = True)\n",
    "output_layer = Dense(len(machine_vocab), activation=softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use these layers $T_y$ times in a `for` loop to generate the outputs, and their parameters will not be reinitialized. You will have to carry out the following steps: \n",
    "\n",
    "1. Propagate the input into a [Bidirectional](https://keras.io/layers/wrappers/#bidirectional) [LSTM](https://keras.io/layers/recurrent/#lstm)\n",
    "2. Iterate for $t = 0, \\dots, T_y-1$: \n",
    "    1. Call `one_step_attention()` on $[\\alpha^{<t,1>},\\alpha^{<t,2>}, ..., \\alpha^{<t,T_x>}]$ and $s^{<t-1>}$ to get the context vector $context^{<t>}$.\n",
    "    2. Give $context^{<t>}$ to the post-attention LSTM cell. Remember pass in the previous hidden-state $s^{\\langle t-1\\rangle}$ and cell-states $c^{\\langle t-1\\rangle}$ of this LSTM using `initial_state= [previous hidden state, previous cell state]`. Get back the new hidden state $s^{<t>}$ and the new cell state $c^{<t>}$.\n",
    "    3. Apply a softmax layer to $s^{<t>}$, get the output. \n",
    "    4. Save the output by adding it to the list of outputs.\n",
    "\n",
    "3. Create your Keras model instance, it should have three inputs (\"inputs\", $s^{<0>}$ and $c^{<0>}$) and output the list of \"outputs\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: model \n",
    "def model(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size,m):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Tx -- length of the input sequence\n",
    "    Ty -- length of the output sequence\n",
    "    n_a -- hidden state size of the Bi-LSTM\n",
    "    n_s -- hidden state size of the post-attention LSTM\n",
    "    human_vocab_size -- size of the python dictionary \"human_vocab\"\n",
    "    machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the inputs of your model with a shape (Tx,)\n",
    "\n",
    "    \n",
    "    X = Input(shape=(Tx, human_vocab_size)) # shape: (m,Tx,human_vocab_size)\n",
    "    \n",
    "    \n",
    "    # Define s0 and c0, initial hidden state for the decoder LSTM of shape (n_s,)\n",
    "    s0 = Input(shape=(n_s,), name='s0')  # shape of s:  (m, 64)\n",
    "    c0 = Input(shape=(n_s,), name='c0')  # shape of c:  (m, 64)\n",
    "    s = s0\n",
    "    c = c0\n",
    "\n",
    "#     m=X.shape[0] # m=None \n",
    "#     s=K.zeros(shape=(m, n_s))\n",
    "#     c=K.zeros(shape=(m, n_s))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Initialize empty list of outputs\n",
    "    outputs = []\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Step 1: Define your pre-attention Bi-LSTM. Remember to use return_sequences=True. (≈ 1 line)\n",
    "    a, forward_h, forward_c, backward_h, backward_c= pre_activation_LSTM_cell(inputs=X) #  shape of a : (m,Tx, 2*n_a)\n",
    "\n",
    "    # Step 2: Iterate for Ty steps\n",
    "    for t in range(Ty):\n",
    "    \n",
    "        # Step 2.A: Perform one step of the attention mechanism to get back the context vector at step t (≈ 1 line)\n",
    "        # a.shape()\n",
    "        context = one_step_attention(a, s) # shape of s:  (m, 64)\n",
    "        \n",
    "        # Step 2.B: Apply the post-attention LSTM cell to the \"context\" vector.\n",
    "        # Don't forget to pass: initial_state = [hidden state, cell state] (≈ 1 line)\n",
    "        \n",
    "        s, _, c = post_activation_LSTM_cell(inputs=context,initial_state=[s, c])#输入只有一个时间步\n",
    "\n",
    "        \n",
    "        # Step 2.C: Apply Dense layer to the hidden state output of the post-attention LSTM (≈ 1 line)\n",
    "        out = output_layer(s)\n",
    "        \n",
    "        # Step 2.D: Append \"out\" to the \"outputs\" list (≈ 1 line)\n",
    "        outputs.append(out) # shape of out : ( m ,machine_vocab) \n",
    "    \n",
    "    #\n",
    "    # Step 3: Create model instance taking three inputs and returning the list of outputs. (≈ 1 line)\n",
    "    model =  Model(inputs=[X,s0, c0], outputs=outputs)\n",
    "    #shape of outs : ( Ty ,m ,machine_vocab) \n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   By XRH in 2019.12.16\n",
    "#  decoder 中，修改 LSTM 的初始 隐藏状态的输入 ，由原来的 0 向量，改为 encoder 中 LSTM 最后一个时间步的隐状态（注意进行拼接）    \n",
    "#\n",
    "def model1(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Tx -- length of the input sequence\n",
    "    Ty -- length of the output sequence\n",
    "    n_a -- hidden state size of the Bi-LSTM\n",
    "    n_s -- hidden state size of the post-attention LSTM\n",
    "    human_vocab_size -- size of the python dictionary \"human_vocab\"\n",
    "    machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the inputs of your model with a shape (Tx,)\n",
    "\n",
    "    \n",
    "    X = Input(shape=(Tx, human_vocab_size)) # shape: (m,Tx,human_vocab_size)\n",
    "    \n",
    "    \n",
    "    # Initialize empty list of outputs\n",
    "    outputs = []\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Step 1: Define your pre-attention Bi-LSTM. Remember to use return_sequences=True. (≈ 1 line)\n",
    "    a, forward_h, forward_c, backward_h, backward_c= pre_activation_LSTM_cell(inputs=X) #  shape of a : (m,Tx, 2*n_a)\n",
    "    s = Concatenate()([forward_h, backward_h]) # shape of s:  (m, 64)\n",
    "    c = Concatenate()([forward_c, backward_c]) # shape of c:  (m, 64)\n",
    "\n",
    "\n",
    "    # Step 2: Iterate for Ty steps\n",
    "    for t in range(Ty):\n",
    "    \n",
    "        # Step 2.A: Perform one step of the attention mechanism to get back the context vector at step t (≈ 1 line)\n",
    "        # a.shape()\n",
    "        context = one_step_attention(a, s) # shape of s:  (m, 64)\n",
    "        \n",
    "        # Step 2.B: Apply the post-attention LSTM cell to the \"context\" vector.\n",
    "        # Don't forget to pass: initial_state = [hidden state, cell state] (≈ 1 line)\n",
    "        \n",
    "        s, _, c = post_activation_LSTM_cell(inputs=context,initial_state=[s, c])#输入只有一个时间步\n",
    "\n",
    "        \n",
    "        # Step 2.C: Apply Dense layer to the hidden state output of the post-attention LSTM (≈ 1 line)\n",
    "        out = output_layer(s)\n",
    "        \n",
    "        # Step 2.D: Append \"out\" to the \"outputs\" list (≈ 1 line)\n",
    "        outputs.append(out) # shape of out : ( m ,machine_vocab) \n",
    "    \n",
    "    #\n",
    "    # Step 3: Create model instance taking three inputs and returning the list of outputs. (≈ 1 line)\n",
    "    model =  Model(inputs=[X], outputs=outputs)\n",
    "    #shape of outs : ( Ty ,m ,machine_vocab) \n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"pred_4:0\", shape=(?, 11), dtype=float32)\n",
      "Tensor(\"lambda_5/Reshape:0\", shape=(1, 11), dtype=float32)\n",
      "Tensor(\"reshape_5/Reshape:0\", shape=(?, 1, 11), dtype=float32)\n",
      "Tensor(\"pred_5:0\", shape=(?, 11), dtype=float32)\n",
      "Tensor(\"lambda_6/ArgMax:0\", shape=(?,), dtype=int64)\n",
      "Tensor(\"lambda_7/one_hot:0\", shape=(?, 11), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# testing0\n",
    "def testing_tensors():\n",
    "    a=K.zeros(5) \n",
    "    print(a)\n",
    "    print(K.eval(a))\n",
    "\n",
    "    b=K.constant([1, 2, 3],dtype='uint8')\n",
    "    print(b)\n",
    "    print(K.eval(b))\n",
    "\n",
    "    val = np.array([[1, 2], [3, 4]])\n",
    "    kvar = K.variable(value=val, dtype='int32', name='example_var')\n",
    "    print(kvar)\n",
    "    print(K.eval(kvar))\n",
    "    \n",
    "    keras_placeholder = K.placeholder(shape=(2, 4, 5))\n",
    "    print(keras_placeholder)\n",
    "\n",
    "\n",
    "# testing_tensors()\n",
    "\n",
    "# testing1 \n",
    "def convert_to_one_hot(y, C):\n",
    "    return np.eye(C)[y.reshape(-1)].T\n",
    "\n",
    "# y = np.array([1,2,3,4])\n",
    "# y=np.argmax(y, axis = -1)\n",
    "# convert_to_one_hot(y,len(machine_vocab))\n",
    "\n",
    "# testing2\n",
    "# y=np.array([1,2,3,4])\n",
    "# y\n",
    "# y.reshape(2,2)\n",
    "# y\n",
    "\n",
    "# testing3   \n",
    "#array的尊卑关系： numpy -> backend tensor -> layer tensor\n",
    "def tensor_test():\n",
    "    \n",
    "#0.\n",
    "    x=np.zeros((1,len(machine_vocab)))\n",
    "    print(x.shape)\n",
    "    pred0=K.reshape(x, (1,len(machine_vocab)))\n",
    "    print(pred0)\n",
    "#     Tensor(\"Reshape_2:0\", shape=(1, 11), dtype=float64)\n",
    "\n",
    "#1.    \n",
    "#     pred0=np.zeros((1,len(machine_vocab))) \n",
    "#     pred0=Reshape(target_shape=(1,len(machine_vocab)))(pred0)\n",
    "#   ValueError: Layer reshape_42 was called with an input that isn't a symbolic tensor. Received type: <class 'numpy.ndarray'>. Full input: [array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])]. All inputs to the layer should be tensors.     \n",
    "\n",
    "#2.   \n",
    "#     pred0=K.zeros((1,len(machine_vocab))) \n",
    "#     pred0=Reshape(target_shape=(1,len(machine_vocab)))(pred0)\n",
    "#     print(pred0)\n",
    "#  Tensor(\"reshape_91/Reshape:0\", shape=(1, 1, 11), dtype=float32)\n",
    "\n",
    "# tensor_test()\n",
    "    \n",
    "\n",
    "def reshape_tensor(x, shape):\n",
    "    return K.reshape(x, shape);\n",
    "\n",
    "def argmax_tensor(x, axis):\n",
    "    return K.argmax(x, axis);\n",
    "\n",
    "def one_hot_tensor(x, num_classes):\n",
    "    #by # https://fdalvi.github.io/blog/2018-04-07-keras-sequential-onehot/\n",
    "    return K.one_hot(K.cast(x, 'uint8'), num_classes);\n",
    "\n",
    "\n",
    "\n",
    "# testing4\n",
    "def tensor_test2():\n",
    "    pred=Input(shape=(len(machine_vocab),), name='pred')  \n",
    "    print (pred)\n",
    "\n",
    "#   以下两个 reshape 效果相同 都是输出了 layer tensor\n",
    "    x = Lambda(reshape_tensor, arguments={'shape': (1, len(machine_vocab))})(pred)\n",
    "    print (x)\n",
    "    \n",
    "    y=Reshape(target_shape=(1,len(machine_vocab)))(pred)\n",
    "    print (y)\n",
    "\n",
    "tensor_test2()\n",
    "    \n",
    "def tensor_test3():\n",
    "    pred=Input(shape=(len(machine_vocab),), name='pred')  \n",
    "    print (pred)\n",
    "\n",
    "    pred=Lambda(argmax_tensor, arguments={'axis': -1 })(pred)\n",
    "    print(pred)\n",
    "    pred=Lambda(one_hot_tensor, arguments={'num_classes': len(machine_vocab) })(pred)  \n",
    "    print(pred)\n",
    "\n",
    "tensor_test3()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"dense_1/BiasAdd:0\", shape=(?, 1), dtype=float32)\n",
      "WARNING:tensorflow:From E:\\anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From E:\\anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/5\n",
      "100/100 [==============================] - ETA: 6s - loss: 3.147 - 1s 7ms/step - loss: 7.1115\n",
      "Epoch 2/5\n",
      "100/100 [==============================] - ETA: 0s - loss: 6.133 - 0s 340us/step - loss: 7.6666\n",
      "Epoch 3/5\n",
      "100/100 [==============================] - ETA: 0s - loss: 7.666 - 0s 310us/step - loss: 7.6666\n",
      "Epoch 4/5\n",
      "100/100 [==============================] - ETA: 0s - loss: 6.133 - 0s 290us/step - loss: 7.6666\n",
      "Epoch 5/5\n",
      "100/100 [==============================] - ETA: 0s - loss: 10.73 - 0s 300us/step - loss: 7.6666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2480ea69eb8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import losses\n",
    "def base_model(m):\n",
    "\n",
    "    # 建立模型第一层的 keras Tensor，只能是Input 或者从 别的地方传来的 tensor from keras.layer\n",
    "    X = Input(shape=(32,), name='X') # 此时 shape(None,32) ；真正训练时为 shape(m,32)\n",
    "\n",
    "    init_state = Input(shape=(32,), name='init_state')\n",
    "\n",
    "    #1.\n",
    "    # b= K.zeros((100,32))\n",
    "    # b = Input( name='input_b',tensor=K.zeros((100,32)))  #<tf.Variable 'Variable:0' shape=(100, 32) dtype=float32>\n",
    "    # 喂入模型的一个 batch的样本的数目 m 此时是未知的，因此为None ，\n",
    "    # 所以模型的第一层中，所有和 m 相关的tensor 都要使用 Input，相当于tensorflow的一个占位符，其他方法不work:\n",
    "    #\n",
    "    # 反例1:\n",
    "    # b=K.zeros((init_state.shape[0],32)) # init_state.shape[0]=None，K.zeros 的shape参数 中不能出现None\n",
    "    #\n",
    "    # 反例2:\n",
    "    # b= K.zeros((m,32)) # m 由参数传入，m=100\n",
    "    # b = Input( name='input_b',tensor=K.zeros((m,32))) # <tf.Variable 'Variable:0' shape=(100, 32) dtype=float32>\n",
    "    # 虽然成功建立的 tensor 但是和 来源于 keras.layer 的a (shape: (None,32)) 无法进行 axis=1的拼接\n",
    "    # 即报错： ValueError: A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got inputs shapes: [(None, 32), (100, 32)]\n",
    "\n",
    "    #2.\n",
    "    # 连接的所有tensor 必须来源于 keras.layer（血统纯正），否则无法生成计算图\n",
    "    # 下面两种方法都报 AttributeError: 'NoneType' object has no attribute '_inbound_nodes'\n",
    "    #M1\n",
    "    # init_state_2 = K.constant(np.zeros((1,64)), dtype='float32') \n",
    "    #M2\n",
    "    # init_state_2=K.ones((1,64))\n",
    "    \n",
    "    #M3 使用 keras.layer.Reshape 转换，但是维度不对,会自动的在全面加上一维\n",
    "    \n",
    "\n",
    "    concat = Concatenate(axis=1)([X, init_state])\n",
    "\n",
    "#     concat2= Concatenate(axis=1)([concat, b])\n",
    "\n",
    "#     concat2 = Concatenate(axis=0)([concat, init_state_2])\n",
    "\n",
    "    dense = Dense(1, name='dense_1')\n",
    "\n",
    "    res=dense(concat)\n",
    "\n",
    "    print(res)\n",
    "    model= Model(inputs=[ X , init_state ], outputs=res)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "M=100\n",
    "model = base_model(M)\n",
    "\n",
    "X=np.random.randint(0,10,size=[M,32])\n",
    "init_state=np.zeros((M,32))\n",
    "# b_train=K.zeros((M,32)) #ValueError: If your data is in the form of symbolic tensors, you should specify the `steps_per_epoch` argument (instead of the `batch_size` argument, because symbolic tensors are expected to produce batches of input data).\n",
    "\n",
    "x_train = [X, init_state]\n",
    "y_train=np.random.randint(low=0,high=2,size=[M,1])\n",
    "\n",
    "\n",
    "model.compile(loss=losses.binary_crossentropy, optimizer='sgd')\n",
    "\n",
    "batch_size=10\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=5\n",
    "          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(None, 32), (None, 32)]\n",
      "Tensor(\"X_27:0\", shape=(?, 32), dtype=float32)\n",
      "Tensor(\"init_state_27:0\", shape=(?, 32), dtype=float32)\n",
      "Tensor(\"concat_dense_20/MatMul:0\", shape=(?, 1), dtype=float32)\n",
      "Epoch 1/5\n",
      "100/100 [==============================] - ETA: 1s - loss: 6.389 - 0s 2ms/step - loss: 7.2322\n",
      "Epoch 2/5\n",
      "100/100 [==============================] - ETA: 0s - loss: 9.199 - 0s 330us/step - loss: 7.2066\n",
      "Epoch 3/5\n",
      "100/100 [==============================] - ETA: 0s - loss: 7.666 - 0s 340us/step - loss: 7.2066\n",
      "Epoch 4/5\n",
      "100/100 [==============================] - ETA: 0s - loss: 4.600 - 0s 340us/step - loss: 7.2066\n",
      "Epoch 5/5\n",
      "100/100 [==============================] - ETA: 0s - loss: 7.666 - 0s 310us/step - loss: 7.2066\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x24824f2ca58>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#   By XRH in 2019.12.28\n",
    "# 使用 keras.layer 搭建神经网络 不太灵活：\n",
    "# （1）keras.layer 提供的结构太少，并且只能用这些 层来 搭建\n",
    "# （2）要使用 后端提供的丰富的函数 还需要使用 Lambda层进行包装，较繁琐\n",
    "# 因此，我们 使用 后端的函数来 定义属于自己的层\n",
    "\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "class ConcatDense(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(ConcatDense, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        # 为该层创建一个可训练的权重\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "                                      shape=(input_shape[0][1]+input_shape[1][1], self.output_dim),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        \n",
    "        self.bias = self.add_weight(shape=(input_shape[0][1]+input_shape[1][1],),\n",
    "                                        initializer='zeros',\n",
    "                                        trainable=False)\n",
    "        \n",
    "        self.input_shapes=input_shape\n",
    "        \n",
    "        super(ConcatDense, self).build(input_shape)  # 一定要在最后调用它\n",
    "\n",
    "    def call(self, inputs):\n",
    "        assert isinstance(inputs, list)\n",
    "        \n",
    "        X, init_state = inputs # X shape: (None,32)  init_state shape: (None,32)\n",
    "        \n",
    "        print( self.input_shapes)\n",
    "        print(X)\n",
    "        print(init_state)\n",
    "       \n",
    "        \n",
    "#         a=K.ones(self.input_shapes[0])\n",
    "        \n",
    "        concat=K.concatenate([X,init_state],axis=1)\n",
    "#         concat2=K.concatenate([concat,K.ones((1,64))],axis=0)\n",
    "\n",
    "        outputs= K.bias_add(concat, self.bias, data_format='channels_last')\n",
    "        \n",
    "        outputs=outputs+concat\n",
    "\n",
    "        res=K.dot(outputs, self.kernel)\n",
    "        \n",
    "        return res\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        shape_X, shape_init_state = input_shape\n",
    "        return (shape_X[0], self.output_dim)\n",
    "\n",
    "\n",
    "    \n",
    "from keras import losses\n",
    "def base_model(m):\n",
    "\n",
    "    # 建立模型第一层的 keras Tensor，只能是Input 或者从 别的地方传来的 tensor from keras.layer\n",
    "    X = Input(shape=(32,), name='X') # 此时 shape(None,32) ；真正训练时为 shape(m,32)\n",
    "\n",
    "    init_state = Input(shape=(32,), name='init_state')\n",
    "\n",
    "    res=ConcatDense(1)([ X , init_state ])\n",
    "    print(res)\n",
    "    model= Model(inputs=[ X , init_state ], outputs=res)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "M=100\n",
    "model = base_model(M)\n",
    "\n",
    "X=np.random.randint(0,10,size=[M,32])\n",
    "init_state=np.zeros((M,32))\n",
    "\n",
    "x_train = [X, init_state]\n",
    "y_train=np.random.randint(low=0,high=2,size=[M,1])\n",
    "\n",
    "\n",
    "model.compile(loss=losses.binary_crossentropy, optimizer='sgd')\n",
    "\n",
    "batch_size=10\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=5\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   By XRH in 2019.9.10\n",
    "#   改进attention：解码时加入上一个时刻的输出单词（eg. 当前词是'-' 下一个词必须为数字）\n",
    "#  (1)在decoder， 经过softmax 输出后 取最大的 那一个 machine_vocab 的one-hot \n",
    "#     向量 与 context 拼接后输入 post_activation_LSTM_cell ，\n",
    "#  (2)无需更改 lstm 的输出维度 ，仍然保持 n_s=128\n",
    "# （3）把所有的 layer object 声明为全局的，以便 后面重构 decoder 可以使用训练好的网络结构\n",
    "\n",
    "\n",
    "def argmax_tensor(x, axis):\n",
    "    return K.argmax(x, axis);\n",
    "\n",
    "def argmin_tensor(x, axis):\n",
    "    return K.argmin(x, axis);\n",
    "\n",
    "lambda_argmin=Lambda(argmin_tensor, arguments={'axis': -1 },name='argmin_tensor')\n",
    "\n",
    "\n",
    "def one_hot_tensor(x, num_classes):\n",
    "    #by : https://fdalvi.github.io/blog/2018-04-07-keras-sequential-onehot/\n",
    "    return K.one_hot(K.cast(x, 'uint8'), num_classes);\n",
    "\n",
    "n_a = 64\n",
    "n_s = 128 \n",
    "\n",
    "\n",
    "pre_activation_LSTM_cell=Bidirectional(CuDNNLSTM(n_a, return_sequences=True,return_state = True),name='encoder_lstm')\n",
    "\n",
    "concatenate_s=Concatenate(name='concatenate_s')\n",
    "concatenate_c=Concatenate(name='concatenate_c')\n",
    "\n",
    "concatenate_context=Concatenate()\n",
    "\n",
    "\n",
    "\n",
    "post_activation_LSTM_cell = CuDNNLSTM(n_s, return_state = True,name='decoder_lstm') \n",
    "output_layer = Dense(len(machine_vocab), activation=softmax,name='decoder_output')\n",
    "\n",
    "\n",
    "lambda_argmax=Lambda(argmax_tensor, arguments={'axis': -1 },name='argmax_tensor')\n",
    "lambda_one_hot=Lambda(one_hot_tensor, arguments={'num_classes': len(machine_vocab) },name='one_hot_tensor')  \n",
    "\n",
    "reshape=Reshape(target_shape=(1,len(machine_vocab)))\n",
    "\n",
    "\n",
    "def model2(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Tx -- length of the input sequence\n",
    "    Ty -- length of the output sequence\n",
    "    n_a -- hidden state size of the Bi-LSTM\n",
    "    n_s -- hidden state size of the post-attention LSTM\n",
    "    human_vocab_size -- size of the python dictionary \"human_vocab\"\n",
    "    machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    X = Input(shape=(Tx, human_vocab_size)) # shape: (m,Tx,human_vocab_size)\n",
    "\n",
    "    \n",
    "    pred0=Input(shape=(1,len(machine_vocab)), name='pred0')  # shape of pred0 (m ,1, 11)\n",
    "\n",
    "\n",
    "    pred=pred0\n",
    "    \n",
    "    print('pred: after Input',pred)\n",
    "\n",
    "    outputs = []\n",
    "    \n",
    "  \n",
    "    a, forward_h, forward_c, backward_h, backward_c= pre_activation_LSTM_cell(inputs=X) #  shape of a : (m,Tx, 2*n_a)\n",
    "\n",
    "\n",
    "    s = concatenate_s([forward_h, backward_h]) # shape of s:  (m, 64+64=128)\n",
    "    c = concatenate_c([forward_c, backward_c])\n",
    "\n",
    "\n",
    "    for t in range(Ty):\n",
    "    \n",
    "      \n",
    "#         context = one_step_attention(a, s) # shape of context :  (m, 1, 128)\n",
    "\n",
    "        context=one_step_attention_M2(a, s)\n",
    "        print('context after one_step_attention: ',context)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        context=concatenate_context([context,pred])# shape of context: (m,128+11=139) \n",
    "       \n",
    "        print('context after Concatenate:  ',context)\n",
    "    \n",
    "        s, _, c = post_activation_LSTM_cell(inputs=context,initial_state=[s, c])#输入只有一个时间步\n",
    "\n",
    "    \n",
    "        out = output_layer(s)      \n",
    "       \n",
    " # 1. model 必须全部用 keras 包里的tensor 计算，而不能使用 numpy中的函数，因为都是带上 m个样本 的并行计算（利用GPU加速）      \n",
    "#         pred= np.argmax(out, axis = -1) \n",
    "#         pred=convert_to_one_hot(pred,len(machine_vocab))\n",
    "        \n",
    "\n",
    "#2.也不能 直接用keras.backend 中的函数，它的返回仅仅是tensor \n",
    "#         pred= K.argmax(out, axis = -1)\n",
    "#         pred=K.one_hot(pred,len(machine_vocab))\n",
    "# AttributeError: 'NoneType' object has no attribute '_inbound_nodes'\n",
    "\n",
    "#3.必须用 keras.layers 中的 它的输入输出 自动会考虑 一个batch 的计算 ；注意对比两个 reshape\n",
    "# keras.layers.Reshape(target_shape) 输出为 (batch_size,) + target_shape\n",
    "# keras.backend.reshape(x, shape) 输出为 shape\n",
    "\n",
    "\n",
    "        pred=lambda_argmax(out)\n",
    "        pred=lambda_one_hot(pred)\n",
    "        \n",
    "        print(pred)\n",
    "\n",
    "#         pred=RepeatVector(1)(pred)\n",
    "\n",
    "        pred=reshape(pred)\n",
    "        print(pred)\n",
    "          \n",
    "        outputs.append(out) # shape of out : ( m ,machine_vocab) \n",
    "    \n",
    "    #1.\n",
    "    #model =  Model(inputs=[X, s0, c0], outputs=outputs) \n",
    "    # 未加上 新增的pred0\n",
    "    #ValueError: Graph disconnected: cannot obtain value for tensor Tensor\n",
    "    \n",
    "    #2.\n",
    "\n",
    "    model =  Model(inputs=[X ,pred0], outputs=outputs) \n",
    "    \n",
    "    #shape of outputs : ( Ty ,m ,machine_vocab) \n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to create your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model(Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab),m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = model1(Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = model2(Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model \n",
    "\n",
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model \n",
    "\n",
    "plot_model(model2, to_file='model2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a summary of the model to check if it matches the expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "Here is the summary you should see\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **Total params:**\n",
    "        </td>\n",
    "        <td>\n",
    "         185,484\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **Trainable params:**\n",
    "        </td>\n",
    "        <td>\n",
    "         185,484\n",
    "        </td>\n",
    "    </tr>\n",
    "            <tr>\n",
    "        <td>\n",
    "            **Non-trainable params:**\n",
    "        </td>\n",
    "        <td>\n",
    "         0\n",
    "        </td>\n",
    "    </tr>\n",
    "                    <tr>\n",
    "        <td>\n",
    "            **bidirectional_1's output shape **\n",
    "        </td>\n",
    "        <td>\n",
    "         (None, 30, 128)  \n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **repeat_vector_1's output shape **\n",
    "        </td>\n",
    "        <td>\n",
    "         (None, 30, 128)  \n",
    "        </td>\n",
    "    </tr>\n",
    "                <tr>\n",
    "        <td>\n",
    "            **concatenate_1's output shape **\n",
    "        </td>\n",
    "        <td>\n",
    "         (None, 30, 256) \n",
    "        </td>\n",
    "    </tr>\n",
    "            <tr>\n",
    "        <td>\n",
    "            **attention_weights's output shape **\n",
    "        </td>\n",
    "        <td>\n",
    "         (None, 30, 1)  \n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **dot_1's output shape **\n",
    "        </td>\n",
    "        <td>\n",
    "         (None, 1, 128) \n",
    "        </td>\n",
    "    </tr>\n",
    "           <tr>\n",
    "        <td>\n",
    "            **dense_2's output shape **\n",
    "        </td>\n",
    "        <td>\n",
    "         (None, 11) \n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, after creating your model in Keras, you need to compile it and define what loss, optimizer and metrics your are want to use. Compile your model using `categorical_crossentropy` loss, a custom [Adam](https://keras.io/optimizers/#adam) [optimizer](https://keras.io/optimizers/#usage-of-optimizers) (`learning rate = 0.005`, $\\beta_1 = 0.9$, $\\beta_2 = 0.999$, `decay = 0.01`)  and `['accuracy']` metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ### (≈2 lines)\n",
    "opt = Adam(lr = 0.005, beta_1=0.9, beta_2=0.999, decay = 0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr = 0.005, beta_1=0.9, beta_2=0.999, decay = 0.01)\n",
    "model1.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "opt = Adam(lr = 0.005, beta_1=0.9, beta_2=0.999, decay = 0.01)\n",
    "model2.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to define all your inputs and outputs to fit the model:\n",
    "- You already have X of shape $(m = 10000, T_x = 30)$ containing the training examples.\n",
    "- You need to create `s0` and `c0` to initialize your `post_activation_LSTM_cell` with 0s.\n",
    "- Given the `model()` you coded, you need the \"outputs\" to be a list of T_y elements of shape (m, 11). So that: `outputs[i][0], ..., outputs[i][Ty]` represent the true labels (characters) corresponding to the $i^{th}$ training example (`X[i]`). More generally, `outputs[i][j]` is the true label of the $j^{th}$ character in the $i^{th}$ training example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tips : \n",
    "# n_a = 64\n",
    "# n_s = 128 \n",
    "# m = 10000\n",
    "\n",
    "\n",
    "outputs = list(Yoh.swapaxes(0,1)) #Yoh.swapaxes(0,1) 第0维度 和 第1 维度交换，原来为(m,T_y,11) 变换后 为：(T_y,m,11)\n",
    "\n",
    "# for model\n",
    "s0 = np.zeros((m, n_s)) \n",
    "c0 = np.zeros((m, n_s))\n",
    "\n",
    "\n",
    "# for model2\n",
    "pred0=np.zeros((m,1,len(machine_vocab)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now fit the model and run it for one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/40\n",
      "9000/9000 [==============================] - ETA: 11s - loss: 23.9903 - dense_3_loss: 2.3947 - dense_3_accuracy: 0.0044 - dense_3_accuracy_1: 0.1240 - dense_3_accuracy_2: 0.1875 - dense_3_accuracy_3: 0.0977 - dense_3_accuracy_4: 0.0000e+00 - dense_3_accuracy_5: 0.1021 - dense_3_accuracy_6: 0.1152 - dense_3_accuracy_7: 0.0000e+00 - dense_3_accuracy_8: 0.2681 - dense_3_accuracy_9: 0.10 - ETA: 4s - loss: 23.3284 - dense_3_loss: 2.4286 - dense_3_accuracy: 0.3057 - dense_3_accuracy_1: 0.0620 - dense_3_accuracy_2: 0.1599 - dense_3_accuracy_3: 0.0637 - dense_3_accuracy_4: 0.4617 - dense_3_accuracy_5: 0.0569 - dense_3_accuracy_6: 0.0608 - dense_3_accuracy_7: 0.4968 - dense_3_accuracy_8: 0.1365 - dense_3_accuracy_9: 0.0554         - ETA: 1s - loss: 22.7186 - dense_3_loss: 2.5965 - dense_3_accuracy: 0.4089 - dense_3_accuracy_1: 0.0413 - dense_3_accuracy_2: 0.1066 - dense_3_accuracy_3: 0.0425 - dense_3_accuracy_4: 0.6411 - dense_3_accuracy_5: 0.0379 - dense_3_accuracy_6: 0.0405 - dense_3_accuracy_7: 0.6646 - dense_3_accuracy_8: 0.0910 - dense_3_accuracy_9: 0.036 - ETA: 0s - loss: 22.5230 - dense_3_loss: 2.7942 - dense_3_accuracy: 0.4396 - dense_3_accuracy_1: 0.1340 - dense_3_accuracy_2: 0.1283 - dense_3_accuracy_3: 0.0583 - dense_3_accuracy_4: 0.4808 - dense_3_accuracy_5: 0.2147 - dense_3_accuracy_6: 0.0516 - dense_3_accuracy_7: 0.4984 - dense_3_accuracy_8: 0.1427 - dense_3_accuracy_9: 0.053 - 4s 490us/step - loss: 22.3734 - dense_3_loss: 2.7949 - dense_3_accuracy: 0.4243 - dense_3_accuracy_1: 0.1439 - dense_3_accuracy_2: 0.1358 - dense_3_accuracy_3: 0.0538 - dense_3_accuracy_4: 0.5274 - dense_3_accuracy_5: 0.1954 - dense_3_accuracy_6: 0.0470 - dense_3_accuracy_7: 0.5434 - dense_3_accuracy_8: 0.1299 - dense_3_accuracy_9: 0.0482 - val_loss: 20.9891 - val_dense_3_loss: 2.6126 - val_dense_3_accuracy: 0.0370 - val_dense_3_accuracy_1: 0.2120 - val_dense_3_accuracy_2: 0.1860 - val_dense_3_accuracy_3: 0.0000e+00 - val_dense_3_accuracy_4: 1.0000 - val_dense_3_accuracy_5: 0.0000e+00 - val_dense_3_accuracy_6: 0.0000e+00 - val_dense_3_accuracy_7: 1.0000 - val_dense_3_accuracy_8: 0.0000e+00 - val_dense_3_accuracy_9: 0.0000e+00\n",
      "Epoch 2/40\n",
      "9000/9000 [==============================] - ETA: 0s - loss: 20.9525 - dense_3_loss: 2.6215 - dense_3_accuracy: 0.0439 - dense_3_accuracy_1: 0.2261 - dense_3_accuracy_2: 0.1899 - dense_3_accuracy_3: 0.0000e+00 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.0000e+00 - dense_3_accuracy_6: 0.0000e+00 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.0000e+00 - dense_3_accuracy_9: 0.0000e+0 - ETA: 0s - loss: 20.9099 - dense_3_loss: 2.6115 - dense_3_accuracy: 0.0220 - dense_3_accuracy_1: 0.4131 - dense_3_accuracy_2: 0.2021 - dense_3_accuracy_3: 4.8828e-04 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.0000e+00 - dense_3_accuracy_6: 0.0000e+00 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.0000e+00 - dense_3_accuracy_9: 0.0000e+0 - ETA: 0s - loss: 20.7821 - dense_3_loss: 2.6273 - dense_3_accuracy: 0.0146 - dense_3_accuracy_1: 0.4837 - dense_3_accuracy_2: 0.2163 - dense_3_accuracy_3: 0.0324 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.0000e+00 - dense_3_accuracy_6: 0.0000e+00 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.0000e+00 - dense_3_accuracy_9: 0.0000e+00    - ETA: 0s - loss: 20.6665 - dense_3_loss: 2.6560 - dense_3_accuracy: 0.0110 - dense_3_accuracy_1: 0.5126 - dense_3_accuracy_2: 0.2118 - dense_3_accuracy_3: 0.0468 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.0000e+00 - dense_3_accuracy_6: 0.0000e+00 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.0000e+00 - dense_3_accuracy_9: 0.0000e+0 - 1s 100us/step - loss: 20.5999 - dense_3_loss: 2.6811 - dense_3_accuracy: 0.0100 - dense_3_accuracy_1: 0.5199 - dense_3_accuracy_2: 0.2238 - dense_3_accuracy_3: 0.0520 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.0000e+00 - dense_3_accuracy_6: 0.0000e+00 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.0000e+00 - dense_3_accuracy_9: 0.0000e+00 - val_loss: 19.7951 - val_dense_3_loss: 2.7343 - val_dense_3_accuracy: 0.5780 - val_dense_3_accuracy_1: 0.6190 - val_dense_3_accuracy_2: 0.1820 - val_dense_3_accuracy_3: 0.1090 - val_dense_3_accuracy_4: 0.3360 - val_dense_3_accuracy_5: 0.0000e+00 - val_dense_3_accuracy_6: 0.0000e+00 - val_dense_3_accuracy_7: 1.0000 - val_dense_3_accuracy_8: 0.0000e+00 - val_dense_3_accuracy_9: 0.0000e+00\n",
      "Epoch 3/40\n",
      "9000/9000 [==============================] - ETA: 0s - loss: 19.7571 - dense_3_loss: 2.7297 - dense_3_accuracy: 0.5742 - dense_3_accuracy_1: 0.6133 - dense_3_accuracy_2: 0.1895 - dense_3_accuracy_3: 0.0996 - dense_3_accuracy_4: 0.3340 - dense_3_accuracy_5: 0.0000e+00 - dense_3_accuracy_6: 0.0000e+00 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.0000e+00 - dense_3_accuracy_9: 0.0000e+0 - ETA: 0s - loss: 19.6546 - dense_3_loss: 2.7326 - dense_3_accuracy: 0.5920 - dense_3_accuracy_1: 0.6116 - dense_3_accuracy_2: 0.1956 - dense_3_accuracy_3: 0.1035 - dense_3_accuracy_4: 0.1670 - dense_3_accuracy_5: 0.0042 - dense_3_accuracy_6: 0.0000e+00 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.0000e+00 - dense_3_accuracy_9: 0.0000e+00    - ETA: 0s - loss: 19.5415 - dense_3_loss: 2.7540 - dense_3_accuracy: 0.5889 - dense_3_accuracy_1: 0.6019 - dense_3_accuracy_2: 0.1991 - dense_3_accuracy_3: 0.1038 - dense_3_accuracy_4: 0.1530 - dense_3_accuracy_5: 0.0028 - dense_3_accuracy_6: 0.0000e+00 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.0000e+00 - dense_3_accuracy_9: 0.0000e+0 - ETA: 0s - loss: 19.4110 - dense_3_loss: 2.7704 - dense_3_accuracy: 0.5957 - dense_3_accuracy_1: 0.6055 - dense_3_accuracy_2: 0.1969 - dense_3_accuracy_3: 0.1025 - dense_3_accuracy_4: 0.3647 - dense_3_accuracy_5: 0.0021 - dense_3_accuracy_6: 0.0000e+00 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.0000e+00 - dense_3_accuracy_9: 0.0000e+0 - 1s 100us/step - loss: 19.3430 - dense_3_loss: 2.7809 - dense_3_accuracy: 0.5974 - dense_3_accuracy_1: 0.6063 - dense_3_accuracy_2: 0.2019 - dense_3_accuracy_3: 0.1031 - dense_3_accuracy_4: 0.4218 - dense_3_accuracy_5: 0.0019 - dense_3_accuracy_6: 0.0000e+00 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.0000e+00 - dense_3_accuracy_9: 0.0097 - val_loss: 18.6158 - val_dense_3_loss: 2.7930 - val_dense_3_accuracy: 0.6190 - val_dense_3_accuracy_1: 0.6190 - val_dense_3_accuracy_2: 0.2510 - val_dense_3_accuracy_3: 0.0000e+00 - val_dense_3_accuracy_4: 1.0000 - val_dense_3_accuracy_5: 0.0000e+00 - val_dense_3_accuracy_6: 0.0000e+00 - val_dense_3_accuracy_7: 1.0000 - val_dense_3_accuracy_8: 0.3350 - val_dense_3_accuracy_9: 0.1140\n",
      "Epoch 4/40\n",
      "9000/9000 [==============================] - ETA: 0s - loss: 18.5294 - dense_3_loss: 2.7617 - dense_3_accuracy: 0.6030 - dense_3_accuracy_1: 0.6030 - dense_3_accuracy_2: 0.2739 - dense_3_accuracy_3: 0.0000e+00 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.0000e+00 - dense_3_accuracy_6: 0.0000e+00 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.3218 - dense_3_accuracy_9: 0.120 - ETA: 0s - loss: 18.4812 - dense_3_loss: 2.7430 - dense_3_accuracy: 0.5989 - dense_3_accuracy_1: 0.5989 - dense_3_accuracy_2: 0.2368 - dense_3_accuracy_3: 0.0000e+00 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.0000e+00 - dense_3_accuracy_6: 0.0000e+00 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.3325 - dense_3_accuracy_9: 0.121 - ETA: 0s - loss: 18.3725 - dense_3_loss: 2.7292 - dense_3_accuracy: 0.6053 - dense_3_accuracy_1: 0.6053 - dense_3_accuracy_2: 0.2248 - dense_3_accuracy_3: 1.6276e-04 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.0000e+00 - dense_3_accuracy_6: 0.0000e+00 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.2856 - dense_3_accuracy_9: 0.119 - ETA: 0s - loss: 18.2754 - dense_3_loss: 2.7212 - dense_3_accuracy: 0.6080 - dense_3_accuracy_1: 0.6080 - dense_3_accuracy_2: 0.2207 - dense_3_accuracy_3: 1.2207e-04 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.0000e+00 - dense_3_accuracy_6: 0.0000e+00 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.2145 - dense_3_accuracy_9: 0.115 - 1s 101us/step - loss: 18.2425 - dense_3_loss: 2.7340 - dense_3_accuracy: 0.6063 - dense_3_accuracy_1: 0.6063 - dense_3_accuracy_2: 0.2193 - dense_3_accuracy_3: 1.1111e-04 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.0000e+00 - dense_3_accuracy_6: 0.0000e+00 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.1952 - dense_3_accuracy_9: 0.1107 - val_loss: 17.6991 - val_dense_3_loss: 2.7895 - val_dense_3_accuracy: 0.6190 - val_dense_3_accuracy_1: 0.6190 - val_dense_3_accuracy_2: 0.2190 - val_dense_3_accuracy_3: 0.0000e+00 - val_dense_3_accuracy_4: 1.0000 - val_dense_3_accuracy_5: 0.0000e+00 - val_dense_3_accuracy_6: 0.0000e+00 - val_dense_3_accuracy_7: 1.0000 - val_dense_3_accuracy_8: 0.0110 - val_dense_3_accuracy_9: 0.1070\n",
      "Epoch 5/40\n",
      "9000/9000 [==============================] - ETA: 0s - loss: 17.6225 - dense_3_loss: 2.8104 - dense_3_accuracy: 0.6226 - dense_3_accuracy_1: 0.6226 - dense_3_accuracy_2: 0.2378 - dense_3_accuracy_3: 0.0000e+00 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.0000e+00 - dense_3_accuracy_6: 0.0000e+00 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.0171 - dense_3_accuracy_9: 0.097 - ETA: 0s - loss: 17.5586 - dense_3_loss: 2.7886 - dense_3_accuracy: 0.6128 - dense_3_accuracy_1: 0.6128 - dense_3_accuracy_2: 0.2395 - dense_3_accuracy_3: 0.0000e+00 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.0000e+00 - dense_3_accuracy_6: 0.0000e+00 - dense_3_accuracy_7: 0.9231 - dense_3_accuracy_8: 0.1506 - dense_3_accuracy_9: 0.102 - ETA: 0s - loss: 17.4866 - dense_3_loss: 2.7763 - dense_3_accuracy: 0.6403 - dense_3_accuracy_1: 0.6105 - dense_3_accuracy_2: 0.2786 - dense_3_accuracy_3: 3.2552e-04 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.0000e+00 - dense_3_accuracy_6: 0.0000e+00 - dense_3_accuracy_7: 0.7118 - dense_3_accuracy_8: 0.2106 - dense_3_accuracy_9: 0.100 - ETA: 0s - loss: 17.6103 - dense_3_loss: 2.7772 - dense_3_accuracy: 0.6296 - dense_3_accuracy_1: 0.6073 - dense_3_accuracy_2: 0.2631 - dense_3_accuracy_3: 0.0269 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.0000e+00 - dense_3_accuracy_6: 0.0000e+00 - dense_3_accuracy_7: 0.7838 - dense_3_accuracy_8: 0.1580 - dense_3_accuracy_9: 0.0908    - 1s 101us/step - loss: 17.6139 - dense_3_loss: 2.7538 - dense_3_accuracy: 0.6267 - dense_3_accuracy_1: 0.6249 - dense_3_accuracy_2: 0.2572 - dense_3_accuracy_3: 0.0244 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.0000e+00 - dense_3_accuracy_6: 0.0013 - dense_3_accuracy_7: 0.7134 - dense_3_accuracy_8: 0.1729 - dense_3_accuracy_9: 0.0912 - val_loss: 17.7337 - val_dense_3_loss: 2.5870 - val_dense_3_accuracy: 0.6190 - val_dense_3_accuracy_1: 0.8660 - val_dense_3_accuracy_2: 0.1820 - val_dense_3_accuracy_3: 0.0000e+00 - val_dense_3_accuracy_4: 1.0000 - val_dense_3_accuracy_5: 0.0000e+00 - val_dense_3_accuracy_6: 0.0000e+00 - val_dense_3_accuracy_7: 0.0000e+00 - val_dense_3_accuracy_8: 0.3270 - val_dense_3_accuracy_9: 0.0950\n",
      "Epoch 6/40\n",
      "9000/9000 [==============================] - ETA: 0s - loss: 17.7662 - dense_3_loss: 2.5786 - dense_3_accuracy: 0.6001 - dense_3_accuracy_1: 0.8896 - dense_3_accuracy_2: 0.1968 - dense_3_accuracy_3: 0.0000e+00 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.0000e+00 - dense_3_accuracy_6: 4.8828e-04 - dense_3_accuracy_7: 4.8828e-04 - dense_3_accuracy_8: 0.3457 - dense_3_accuracy_9: 0.093 - ETA: 0s - loss: 17.4457 - dense_3_loss: 2.6036 - dense_3_accuracy: 0.6016 - dense_3_accuracy_1: 0.7463 - dense_3_accuracy_2: 0.2029 - dense_3_accuracy_3: 0.0000e+00 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.0000e+00 - dense_3_accuracy_6: 2.4414e-04 - dense_3_accuracy_7: 0.5002 - dense_3_accuracy_8: 0.1746 - dense_3_accuracy_9: 0.0947    - ETA: 0s - loss: 17.2995 - dense_3_loss: 2.6430 - dense_3_accuracy: 0.6032 - dense_3_accuracy_1: 0.6997 - dense_3_accuracy_2: 0.2021 - dense_3_accuracy_3: 0.0347 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.0000e+00 - dense_3_accuracy_6: 1.6276e-04 - dense_3_accuracy_7: 0.6668 - dense_3_accuracy_8: 0.1164 - dense_3_accuracy_9: 0.0632    - ETA: 0s - loss: 17.2089 - dense_3_loss: 2.6564 - dense_3_accuracy: 0.6060 - dense_3_accuracy_1: 0.6783 - dense_3_accuracy_2: 0.2039 - dense_3_accuracy_3: 0.0559 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.0000e+00 - dense_3_accuracy_6: 1.2207e-04 - dense_3_accuracy_7: 0.7501 - dense_3_accuracy_8: 0.0873 - dense_3_accuracy_9: 0.048 - 1s 100us/step - loss: 17.1517 - dense_3_loss: 2.6508 - dense_3_accuracy: 0.6063 - dense_3_accuracy_1: 0.6722 - dense_3_accuracy_2: 0.2047 - dense_3_accuracy_3: 0.0607 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.0000e+00 - dense_3_accuracy_6: 2.2222e-04 - dense_3_accuracy_7: 0.7710 - dense_3_accuracy_8: 0.0932 - dense_3_accuracy_9: 0.0544 - val_loss: 16.6751 - val_dense_3_loss: 2.6626 - val_dense_3_accuracy: 0.6190 - val_dense_3_accuracy_1: 0.6190 - val_dense_3_accuracy_2: 0.2010 - val_dense_3_accuracy_3: 0.0000e+00 - val_dense_3_accuracy_4: 1.0000 - val_dense_3_accuracy_5: 0.0000e+00 - val_dense_3_accuracy_6: 0.0060 - val_dense_3_accuracy_7: 0.5350 - val_dense_3_accuracy_8: 0.3250 - val_dense_3_accuracy_9: 0.0950\n",
      "Epoch 7/40\n",
      "9000/9000 [==============================] - ETA: 0s - loss: 16.6396 - dense_3_loss: 2.6694 - dense_3_accuracy: 0.6113 - dense_3_accuracy_1: 0.6113 - dense_3_accuracy_2: 0.2002 - dense_3_accuracy_3: 0.0000e+00 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 4.8828e-04 - dense_3_accuracy_6: 0.0073 - dense_3_accuracy_7: 0.5479 - dense_3_accuracy_8: 0.3408 - dense_3_accuracy_9: 0.102 - ETA: 0s - loss: 16.5762 - dense_3_loss: 2.6230 - dense_3_accuracy: 0.6077 - dense_3_accuracy_1: 0.6062 - dense_3_accuracy_2: 0.2053 - dense_3_accuracy_3: 0.0000e+00 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 2.4414e-04 - dense_3_accuracy_6: 0.0051 - dense_3_accuracy_7: 0.6467 - dense_3_accuracy_8: 0.3318 - dense_3_accuracy_9: 0.102 - ETA: 0s - loss: 16.4384 - dense_3_loss: 2.5882 - dense_3_accuracy: 0.6118 - dense_3_accuracy_1: 0.6102 - dense_3_accuracy_2: 0.2111 - dense_3_accuracy_3: 0.0065 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 1.6276e-04 - dense_3_accuracy_6: 0.0034 - dense_3_accuracy_7: 0.7619 - dense_3_accuracy_8: 0.2633 - dense_3_accuracy_9: 0.0985    - ETA: 0s - loss: 16.3328 - dense_3_loss: 2.5720 - dense_3_accuracy: 0.6068 - dense_3_accuracy_1: 0.6056 - dense_3_accuracy_2: 0.2158 - dense_3_accuracy_3: 0.0323 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 1.2207e-04 - dense_3_accuracy_6: 0.0226 - dense_3_accuracy_7: 0.8199 - dense_3_accuracy_8: 0.2019 - dense_3_accuracy_9: 0.104 - 1s 101us/step - loss: 16.2823 - dense_3_loss: 2.5641 - dense_3_accuracy: 0.6074 - dense_3_accuracy_1: 0.6064 - dense_3_accuracy_2: 0.2128 - dense_3_accuracy_3: 0.0388 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.0677 - dense_3_accuracy_6: 0.0300 - dense_3_accuracy_7: 0.8334 - dense_3_accuracy_8: 0.1899 - dense_3_accuracy_9: 0.1090 - val_loss: 15.5526 - val_dense_3_loss: 2.5568 - val_dense_3_accuracy: 0.6190 - val_dense_3_accuracy_1: 0.6190 - val_dense_3_accuracy_2: 0.2030 - val_dense_3_accuracy_3: 0.0480 - val_dense_3_accuracy_4: 1.0000 - val_dense_3_accuracy_5: 0.7830 - val_dense_3_accuracy_6: 0.0860 - val_dense_3_accuracy_7: 0.9110 - val_dense_3_accuracy_8: 0.2990 - val_dense_3_accuracy_9: 0.1140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/40\n",
      "9000/9000 [==============================] - ETA: 0s - loss: 15.5266 - dense_3_loss: 2.5686 - dense_3_accuracy: 0.5933 - dense_3_accuracy_1: 0.5938 - dense_3_accuracy_2: 0.1963 - dense_3_accuracy_3: 0.0581 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.7471 - dense_3_accuracy_6: 0.0942 - dense_3_accuracy_7: 0.9014 - dense_3_accuracy_8: 0.2861 - dense_3_accuracy_9: 0.113 - ETA: 0s - loss: 15.3915 - dense_3_loss: 2.5310 - dense_3_accuracy: 0.5989 - dense_3_accuracy_1: 0.5991 - dense_3_accuracy_2: 0.1997 - dense_3_accuracy_3: 0.0315 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.7520 - dense_3_accuracy_6: 0.0945 - dense_3_accuracy_7: 0.9246 - dense_3_accuracy_8: 0.3032 - dense_3_accuracy_9: 0.122 - ETA: 0s - loss: 15.1674 - dense_3_loss: 2.5113 - dense_3_accuracy: 0.6162 - dense_3_accuracy_1: 0.6071 - dense_3_accuracy_2: 0.2059 - dense_3_accuracy_3: 0.0430 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.7537 - dense_3_accuracy_6: 0.0920 - dense_3_accuracy_7: 0.9491 - dense_3_accuracy_8: 0.2319 - dense_3_accuracy_9: 0.127 - ETA: 0s - loss: 14.9794 - dense_3_loss: 2.5057 - dense_3_accuracy: 0.6826 - dense_3_accuracy_1: 0.6052 - dense_3_accuracy_2: 0.2035 - dense_3_accuracy_3: 0.0574 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.7499 - dense_3_accuracy_6: 0.0936 - dense_3_accuracy_7: 0.9618 - dense_3_accuracy_8: 0.1769 - dense_3_accuracy_9: 0.119 - 1s 102us/step - loss: 14.8689 - dense_3_loss: 2.4937 - dense_3_accuracy: 0.6977 - dense_3_accuracy_1: 0.6064 - dense_3_accuracy_2: 0.2028 - dense_3_accuracy_3: 0.0616 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.7496 - dense_3_accuracy_6: 0.0938 - dense_3_accuracy_7: 0.9652 - dense_3_accuracy_8: 0.1904 - dense_3_accuracy_9: 0.1179 - val_loss: 13.1684 - val_dense_3_loss: 2.3687 - val_dense_3_accuracy: 0.6620 - val_dense_3_accuracy_1: 0.6190 - val_dense_3_accuracy_2: 0.2040 - val_dense_3_accuracy_3: 0.0670 - val_dense_3_accuracy_4: 1.0000 - val_dense_3_accuracy_5: 0.7770 - val_dense_3_accuracy_6: 0.0840 - val_dense_3_accuracy_7: 1.0000 - val_dense_3_accuracy_8: 0.3420 - val_dense_3_accuracy_9: 0.1030\n",
      "Epoch 9/40\n",
      "9000/9000 [==============================] - ETA: 0s - loss: 13.1389 - dense_3_loss: 2.3715 - dense_3_accuracy: 0.6401 - dense_3_accuracy_1: 0.6035 - dense_3_accuracy_2: 0.2075 - dense_3_accuracy_3: 0.0884 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.7485 - dense_3_accuracy_6: 0.0908 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.3584 - dense_3_accuracy_9: 0.100 - ETA: 0s - loss: 12.9065 - dense_3_loss: 2.3513 - dense_3_accuracy: 0.6267 - dense_3_accuracy_1: 0.6045 - dense_3_accuracy_2: 0.2095 - dense_3_accuracy_3: 0.0933 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.7517 - dense_3_accuracy_6: 0.0920 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.3760 - dense_3_accuracy_9: 0.110 - ETA: 0s - loss: 12.7078 - dense_3_loss: 2.3382 - dense_3_accuracy: 0.6748 - dense_3_accuracy_1: 0.6074 - dense_3_accuracy_2: 0.2507 - dense_3_accuracy_3: 0.0964 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.7550 - dense_3_accuracy_6: 0.0918 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.3813 - dense_3_accuracy_9: 0.123 - ETA: 0s - loss: 12.5331 - dense_3_loss: 2.3317 - dense_3_accuracy: 0.7365 - dense_3_accuracy_1: 0.6605 - dense_3_accuracy_2: 0.2836 - dense_3_accuracy_3: 0.1014 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.7520 - dense_3_accuracy_6: 0.0946 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.4019 - dense_3_accuracy_9: 0.122 - 1s 100us/step - loss: 12.4610 - dense_3_loss: 2.3289 - dense_3_accuracy: 0.7448 - dense_3_accuracy_1: 0.6707 - dense_3_accuracy_2: 0.2964 - dense_3_accuracy_3: 0.1031 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.7496 - dense_3_accuracy_6: 0.0954 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.4018 - dense_3_accuracy_9: 0.1231 - val_loss: 11.4897 - val_dense_3_loss: 2.3064 - val_dense_3_accuracy: 0.9110 - val_dense_3_accuracy_1: 0.9160 - val_dense_3_accuracy_2: 0.2840 - val_dense_3_accuracy_3: 0.1140 - val_dense_3_accuracy_4: 1.0000 - val_dense_3_accuracy_5: 0.7910 - val_dense_3_accuracy_6: 0.1030 - val_dense_3_accuracy_7: 1.0000 - val_dense_3_accuracy_8: 0.3800 - val_dense_3_accuracy_9: 0.1300\n",
      "Epoch 10/40\n",
      "9000/9000 [==============================] - ETA: 0s - loss: 11.4868 - dense_3_loss: 2.3246 - dense_3_accuracy: 0.9307 - dense_3_accuracy_1: 0.9360 - dense_3_accuracy_2: 0.3232 - dense_3_accuracy_3: 0.1118 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.7549 - dense_3_accuracy_6: 0.1230 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.3638 - dense_3_accuracy_9: 0.124 - ETA: 0s - loss: 11.3657 - dense_3_loss: 2.3139 - dense_3_accuracy: 0.9204 - dense_3_accuracy_1: 0.9241 - dense_3_accuracy_2: 0.3547 - dense_3_accuracy_3: 0.1123 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.7688 - dense_3_accuracy_6: 0.1377 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.3674 - dense_3_accuracy_9: 0.124 - ETA: 0s - loss: 11.3440 - dense_3_loss: 2.3117 - dense_3_accuracy: 0.8818 - dense_3_accuracy_1: 0.8861 - dense_3_accuracy_2: 0.3503 - dense_3_accuracy_3: 0.1234 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.7721 - dense_3_accuracy_6: 0.1463 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.3898 - dense_3_accuracy_9: 0.129 - ETA: 0s - loss: 11.5831 - dense_3_loss: 2.3209 - dense_3_accuracy: 0.8301 - dense_3_accuracy_1: 0.8309 - dense_3_accuracy_2: 0.3262 - dense_3_accuracy_3: 0.1221 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.7767 - dense_3_accuracy_6: 0.1547 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.3907 - dense_3_accuracy_9: 0.126 - 1s 101us/step - loss: 11.6384 - dense_3_loss: 2.3208 - dense_3_accuracy: 0.8136 - dense_3_accuracy_1: 0.8138 - dense_3_accuracy_2: 0.3153 - dense_3_accuracy_3: 0.1200 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.7788 - dense_3_accuracy_6: 0.1547 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.3863 - dense_3_accuracy_9: 0.1269 - val_loss: 11.7915 - val_dense_3_loss: 2.3087 - val_dense_3_accuracy: 0.6660 - val_dense_3_accuracy_1: 0.6590 - val_dense_3_accuracy_2: 0.2430 - val_dense_3_accuracy_3: 0.1240 - val_dense_3_accuracy_4: 1.0000 - val_dense_3_accuracy_5: 0.7930 - val_dense_3_accuracy_6: 0.1340 - val_dense_3_accuracy_7: 1.0000 - val_dense_3_accuracy_8: 0.3360 - val_dense_3_accuracy_9: 0.1490\n",
      "Epoch 11/40\n",
      "9000/9000 [==============================] - ETA: 0s - loss: 11.8491 - dense_3_loss: 2.3053 - dense_3_accuracy: 0.6543 - dense_3_accuracy_1: 0.6470 - dense_3_accuracy_2: 0.2520 - dense_3_accuracy_3: 0.1074 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.7583 - dense_3_accuracy_6: 0.1470 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.3423 - dense_3_accuracy_9: 0.137 - ETA: 0s - loss: 11.6091 - dense_3_loss: 2.3104 - dense_3_accuracy: 0.7471 - dense_3_accuracy_1: 0.6995 - dense_3_accuracy_2: 0.2747 - dense_3_accuracy_3: 0.1130 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.7676 - dense_3_accuracy_6: 0.1663 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.3679 - dense_3_accuracy_9: 0.127 - ETA: 0s - loss: 11.6315 - dense_3_loss: 2.3267 - dense_3_accuracy: 0.6390 - dense_3_accuracy_1: 0.6891 - dense_3_accuracy_2: 0.3050 - dense_3_accuracy_3: 0.1164 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.7793 - dense_3_accuracy_6: 0.1637 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.3828 - dense_3_accuracy_9: 0.122 - ETA: 0s - loss: 11.6131 - dense_3_loss: 2.3251 - dense_3_accuracy: 0.5836 - dense_3_accuracy_1: 0.6554 - dense_3_accuracy_2: 0.2815 - dense_3_accuracy_3: 0.1191 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.7876 - dense_3_accuracy_6: 0.1654 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.3882 - dense_3_accuracy_9: 0.125 - 1s 101us/step - loss: 11.5638 - dense_3_loss: 2.3145 - dense_3_accuracy: 0.6063 - dense_3_accuracy_1: 0.6749 - dense_3_accuracy_2: 0.2904 - dense_3_accuracy_3: 0.1177 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.7911 - dense_3_accuracy_6: 0.1670 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.3893 - dense_3_accuracy_9: 0.1259 - val_loss: 10.8501 - val_dense_3_loss: 2.2971 - val_dense_3_accuracy: 0.8740 - val_dense_3_accuracy_1: 0.8970 - val_dense_3_accuracy_2: 0.4040 - val_dense_3_accuracy_3: 0.1160 - val_dense_3_accuracy_4: 1.0000 - val_dense_3_accuracy_5: 0.8440 - val_dense_3_accuracy_6: 0.1560 - val_dense_3_accuracy_7: 1.0000 - val_dense_3_accuracy_8: 0.3850 - val_dense_3_accuracy_9: 0.1150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/40\n",
      "9000/9000 [==============================] - ETA: 0s - loss: 10.8414 - dense_3_loss: 2.3121 - dense_3_accuracy: 0.8735 - dense_3_accuracy_1: 0.9028 - dense_3_accuracy_2: 0.3940 - dense_3_accuracy_3: 0.1235 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.8013 - dense_3_accuracy_6: 0.1812 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.3862 - dense_3_accuracy_9: 0.135 - ETA: 0s - loss: 10.7965 - dense_3_loss: 2.3053 - dense_3_accuracy: 0.8582 - dense_3_accuracy_1: 0.8860 - dense_3_accuracy_2: 0.3833 - dense_3_accuracy_3: 0.1201 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.8113 - dense_3_accuracy_6: 0.1741 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.3860 - dense_3_accuracy_9: 0.133 - ETA: 0s - loss: 10.6761 - dense_3_loss: 2.3008 - dense_3_accuracy: 0.8848 - dense_3_accuracy_1: 0.9071 - dense_3_accuracy_2: 0.3953 - dense_3_accuracy_3: 0.1242 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.8153 - dense_3_accuracy_6: 0.1707 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.3848 - dense_3_accuracy_9: 0.136 - ETA: 0s - loss: 10.6958 - dense_3_loss: 2.2990 - dense_3_accuracy: 0.8947 - dense_3_accuracy_1: 0.9069 - dense_3_accuracy_2: 0.3955 - dense_3_accuracy_3: 0.1237 - dense_3_accuracy_4: 0.9939 - dense_3_accuracy_5: 0.8240 - dense_3_accuracy_6: 0.1760 - dense_3_accuracy_7: 0.9948 - dense_3_accuracy_8: 0.3872 - dense_3_accuracy_9: 0.136 - 1s 101us/step - loss: 10.6655 - dense_3_loss: 2.2955 - dense_3_accuracy: 0.8980 - dense_3_accuracy_1: 0.9081 - dense_3_accuracy_2: 0.3981 - dense_3_accuracy_3: 0.1234 - dense_3_accuracy_4: 0.9944 - dense_3_accuracy_5: 0.8196 - dense_3_accuracy_6: 0.1797 - dense_3_accuracy_7: 0.9952 - dense_3_accuracy_8: 0.3856 - dense_3_accuracy_9: 0.1347 - val_loss: 10.4298 - val_dense_3_loss: 2.2699 - val_dense_3_accuracy: 0.9340 - val_dense_3_accuracy_1: 0.9380 - val_dense_3_accuracy_2: 0.3800 - val_dense_3_accuracy_3: 0.1330 - val_dense_3_accuracy_4: 1.0000 - val_dense_3_accuracy_5: 0.7830 - val_dense_3_accuracy_6: 0.1810 - val_dense_3_accuracy_7: 1.0000 - val_dense_3_accuracy_8: 0.3880 - val_dense_3_accuracy_9: 0.1390\n",
      "Epoch 13/40\n",
      "9000/9000 [==============================] - ETA: 0s - loss: 10.3979 - dense_3_loss: 2.2724 - dense_3_accuracy: 0.9492 - dense_3_accuracy_1: 0.9492 - dense_3_accuracy_2: 0.4023 - dense_3_accuracy_3: 0.1108 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.7583 - dense_3_accuracy_6: 0.1924 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.3955 - dense_3_accuracy_9: 0.151 - ETA: 0s - loss: 10.3401 - dense_3_loss: 2.2775 - dense_3_accuracy: 0.9443 - dense_3_accuracy_1: 0.9485 - dense_3_accuracy_2: 0.4221 - dense_3_accuracy_3: 0.1143 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.7634 - dense_3_accuracy_6: 0.1897 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.4221 - dense_3_accuracy_9: 0.141 - ETA: 0s - loss: 10.2933 - dense_3_loss: 2.2748 - dense_3_accuracy: 0.9422 - dense_3_accuracy_1: 0.9478 - dense_3_accuracy_2: 0.4359 - dense_3_accuracy_3: 0.1185 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.7689 - dense_3_accuracy_6: 0.1943 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.4137 - dense_3_accuracy_9: 0.143 - ETA: 0s - loss: 10.2727 - dense_3_loss: 2.2719 - dense_3_accuracy: 0.9426 - dense_3_accuracy_1: 0.9473 - dense_3_accuracy_2: 0.4349 - dense_3_accuracy_3: 0.1229 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.7792 - dense_3_accuracy_6: 0.1945 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.4170 - dense_3_accuracy_9: 0.150 - 1s 100us/step - loss: 10.2570 - dense_3_loss: 2.2724 - dense_3_accuracy: 0.9441 - dense_3_accuracy_1: 0.9470 - dense_3_accuracy_2: 0.4359 - dense_3_accuracy_3: 0.1232 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.7792 - dense_3_accuracy_6: 0.1987 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.4163 - dense_3_accuracy_9: 0.1486 - val_loss: 10.0449 - val_dense_3_loss: 2.2677 - val_dense_3_accuracy: 0.9440 - val_dense_3_accuracy_1: 0.9410 - val_dense_3_accuracy_2: 0.4130 - val_dense_3_accuracy_3: 0.1520 - val_dense_3_accuracy_4: 1.0000 - val_dense_3_accuracy_5: 0.7890 - val_dense_3_accuracy_6: 0.2000 - val_dense_3_accuracy_7: 1.0000 - val_dense_3_accuracy_8: 0.4200 - val_dense_3_accuracy_9: 0.1430\n",
      "Epoch 14/40\n",
      "9000/9000 [==============================] - ETA: 0s - loss: 10.0128 - dense_3_loss: 2.2680 - dense_3_accuracy: 0.9526 - dense_3_accuracy_1: 0.9478 - dense_3_accuracy_2: 0.3950 - dense_3_accuracy_3: 0.1440 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.7603 - dense_3_accuracy_6: 0.2119 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.4272 - dense_3_accuracy_9: 0.138 - ETA: 0s - loss: 9.9991 - dense_3_loss: 2.2677 - dense_3_accuracy: 0.9512 - dense_3_accuracy_1: 0.9492 - dense_3_accuracy_2: 0.4001 - dense_3_accuracy_3: 0.1328 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.7559 - dense_3_accuracy_6: 0.2119 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.4216 - dense_3_accuracy_9: 0.135 - ETA: 0s - loss: 9.9830 - dense_3_loss: 2.2685 - dense_3_accuracy: 0.9523 - dense_3_accuracy_1: 0.9505 - dense_3_accuracy_2: 0.4046 - dense_3_accuracy_3: 0.1310 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.7635 - dense_3_accuracy_6: 0.2048 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.4131 - dense_3_accuracy_9: 0.13 - ETA: 0s - loss: 9.9638 - dense_3_loss: 2.2706 - dense_3_accuracy: 0.9528 - dense_3_accuracy_1: 0.9507 - dense_3_accuracy_2: 0.4136 - dense_3_accuracy_3: 0.1296 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.7849 - dense_3_accuracy_6: 0.1993 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.4070 - dense_3_accuracy_9: 0.13 - 1s 100us/step - loss: 9.9464 - dense_3_loss: 2.2690 - dense_3_accuracy: 0.9531 - dense_3_accuracy_1: 0.9511 - dense_3_accuracy_2: 0.4177 - dense_3_accuracy_3: 0.1281 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.7894 - dense_3_accuracy_6: 0.1979 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.4089 - dense_3_accuracy_9: 0.1336 - val_loss: 9.8364 - val_dense_3_loss: 2.2570 - val_dense_3_accuracy: 0.9270 - val_dense_3_accuracy_1: 0.9290 - val_dense_3_accuracy_2: 0.4710 - val_dense_3_accuracy_3: 0.1390 - val_dense_3_accuracy_4: 1.0000 - val_dense_3_accuracy_5: 0.8340 - val_dense_3_accuracy_6: 0.1830 - val_dense_3_accuracy_7: 1.0000 - val_dense_3_accuracy_8: 0.4090 - val_dense_3_accuracy_9: 0.1430\n",
      "Epoch 15/40\n",
      "9000/9000 [==============================] - ETA: 0s - loss: 9.7682 - dense_3_loss: 2.2650 - dense_3_accuracy: 0.9419 - dense_3_accuracy_1: 0.9453 - dense_3_accuracy_2: 0.4517 - dense_3_accuracy_3: 0.1348 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.8081 - dense_3_accuracy_6: 0.2070 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.4272 - dense_3_accuracy_9: 0.14 - ETA: 0s - loss: 9.7435 - dense_3_loss: 2.2680 - dense_3_accuracy: 0.9500 - dense_3_accuracy_1: 0.9512 - dense_3_accuracy_2: 0.4780 - dense_3_accuracy_3: 0.1294 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.8079 - dense_3_accuracy_6: 0.2019 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.4155 - dense_3_accuracy_9: 0.14 - ETA: 0s - loss: 9.7302 - dense_3_loss: 2.2694 - dense_3_accuracy: 0.9510 - dense_3_accuracy_1: 0.9526 - dense_3_accuracy_2: 0.4764 - dense_3_accuracy_3: 0.1234 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.8197 - dense_3_accuracy_6: 0.2002 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.4245 - dense_3_accuracy_9: 0.14 - ETA: 0s - loss: 9.7070 - dense_3_loss: 2.2705 - dense_3_accuracy: 0.9515 - dense_3_accuracy_1: 0.9525 - dense_3_accuracy_2: 0.4801 - dense_3_accuracy_3: 0.1228 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.8210 - dense_3_accuracy_6: 0.2023 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.4271 - dense_3_accuracy_9: 0.14 - 1s 100us/step - loss: 9.6973 - dense_3_loss: 2.2691 - dense_3_accuracy: 0.9520 - dense_3_accuracy_1: 0.9528 - dense_3_accuracy_2: 0.4798 - dense_3_accuracy_3: 0.1214 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.8186 - dense_3_accuracy_6: 0.2053 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.4258 - dense_3_accuracy_9: 0.1476 - val_loss: 9.6138 - val_dense_3_loss: 2.2665 - val_dense_3_accuracy: 0.9340 - val_dense_3_accuracy_1: 0.9390 - val_dense_3_accuracy_2: 0.4690 - val_dense_3_accuracy_3: 0.1380 - val_dense_3_accuracy_4: 1.0000 - val_dense_3_accuracy_5: 0.8450 - val_dense_3_accuracy_6: 0.1870 - val_dense_3_accuracy_7: 1.0000 - val_dense_3_accuracy_8: 0.4200 - val_dense_3_accuracy_9: 0.1430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/40\n",
      "9000/9000 [==============================] - ETA: 0s - loss: 9.5056 - dense_3_loss: 2.2811 - dense_3_accuracy: 0.9634 - dense_3_accuracy_1: 0.9658 - dense_3_accuracy_2: 0.4873 - dense_3_accuracy_3: 0.1147 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.8159 - dense_3_accuracy_6: 0.2144 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.4185 - dense_3_accuracy_9: 0.14 - ETA: 0s - loss: 9.5127 - dense_3_loss: 2.2807 - dense_3_accuracy: 0.9592 - dense_3_accuracy_1: 0.9595 - dense_3_accuracy_2: 0.4932 - dense_3_accuracy_3: 0.1289 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.8352 - dense_3_accuracy_6: 0.1990 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.4412 - dense_3_accuracy_9: 0.14 - ETA: 0s - loss: 9.5113 - dense_3_loss: 2.2754 - dense_3_accuracy: 0.9567 - dense_3_accuracy_1: 0.9562 - dense_3_accuracy_2: 0.4992 - dense_3_accuracy_3: 0.1294 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.8340 - dense_3_accuracy_6: 0.2101 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.4351 - dense_3_accuracy_9: 0.14 - ETA: 0s - loss: 9.4960 - dense_3_loss: 2.2744 - dense_3_accuracy: 0.9572 - dense_3_accuracy_1: 0.9565 - dense_3_accuracy_2: 0.5001 - dense_3_accuracy_3: 0.1271 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.8406 - dense_3_accuracy_6: 0.2118 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.4336 - dense_3_accuracy_9: 0.14 - 1s 100us/step - loss: 9.4912 - dense_3_loss: 2.2714 - dense_3_accuracy: 0.9571 - dense_3_accuracy_1: 0.9564 - dense_3_accuracy_2: 0.5007 - dense_3_accuracy_3: 0.1279 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.8422 - dense_3_accuracy_6: 0.2096 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.4348 - dense_3_accuracy_9: 0.1450 - val_loss: 9.4501 - val_dense_3_loss: 2.2573 - val_dense_3_accuracy: 0.9540 - val_dense_3_accuracy_1: 0.9500 - val_dense_3_accuracy_2: 0.5230 - val_dense_3_accuracy_3: 0.1550 - val_dense_3_accuracy_4: 1.0000 - val_dense_3_accuracy_5: 0.8830 - val_dense_3_accuracy_6: 0.2120 - val_dense_3_accuracy_7: 1.0000 - val_dense_3_accuracy_8: 0.4110 - val_dense_3_accuracy_9: 0.1480\n",
      "Epoch 17/40\n",
      "9000/9000 [==============================] - ETA: 0s - loss: 9.4525 - dense_3_loss: 2.2660 - dense_3_accuracy: 0.9551 - dense_3_accuracy_1: 0.9556 - dense_3_accuracy_2: 0.5132 - dense_3_accuracy_3: 0.1338 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.8662 - dense_3_accuracy_6: 0.2012 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.3848 - dense_3_accuracy_9: 0.17 - ETA: 0s - loss: 9.3758 - dense_3_loss: 2.2628 - dense_3_accuracy: 0.9585 - dense_3_accuracy_1: 0.9585 - dense_3_accuracy_2: 0.5073 - dense_3_accuracy_3: 0.1301 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.8779 - dense_3_accuracy_6: 0.2148 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.4102 - dense_3_accuracy_9: 0.15 - ETA: 0s - loss: 9.3596 - dense_3_loss: 2.2637 - dense_3_accuracy: 0.9580 - dense_3_accuracy_1: 0.9577 - dense_3_accuracy_2: 0.5127 - dense_3_accuracy_3: 0.1366 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.8789 - dense_3_accuracy_6: 0.2126 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.4255 - dense_3_accuracy_9: 0.16 - ETA: 0s - loss: 9.3348 - dense_3_loss: 2.2648 - dense_3_accuracy: 0.9609 - dense_3_accuracy_1: 0.9602 - dense_3_accuracy_2: 0.5101 - dense_3_accuracy_3: 0.1381 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.8812 - dense_3_accuracy_6: 0.2239 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.4330 - dense_3_accuracy_9: 0.16 - 1s 100us/step - loss: 9.3169 - dense_3_loss: 2.2602 - dense_3_accuracy: 0.9613 - dense_3_accuracy_1: 0.9604 - dense_3_accuracy_2: 0.5106 - dense_3_accuracy_3: 0.1366 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.8819 - dense_3_accuracy_6: 0.2263 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.4343 - dense_3_accuracy_9: 0.1606 - val_loss: 9.2772 - val_dense_3_loss: 2.2541 - val_dense_3_accuracy: 0.9610 - val_dense_3_accuracy_1: 0.9600 - val_dense_3_accuracy_2: 0.4890 - val_dense_3_accuracy_3: 0.1640 - val_dense_3_accuracy_4: 1.0000 - val_dense_3_accuracy_5: 0.9160 - val_dense_3_accuracy_6: 0.2240 - val_dense_3_accuracy_7: 1.0000 - val_dense_3_accuracy_8: 0.4540 - val_dense_3_accuracy_9: 0.1400\n",
      "Epoch 18/40\n",
      "9000/9000 [==============================] - ETA: 0s - loss: 9.2347 - dense_3_loss: 2.2682 - dense_3_accuracy: 0.9629 - dense_3_accuracy_1: 0.9619 - dense_3_accuracy_2: 0.5137 - dense_3_accuracy_3: 0.1650 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.8901 - dense_3_accuracy_6: 0.2334 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.4468 - dense_3_accuracy_9: 0.16 - ETA: 0s - loss: 9.2062 - dense_3_loss: 2.2619 - dense_3_accuracy: 0.9617 - dense_3_accuracy_1: 0.9607 - dense_3_accuracy_2: 0.5054 - dense_3_accuracy_3: 0.1531 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.8982 - dense_3_accuracy_6: 0.2468 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.4497 - dense_3_accuracy_9: 0.15 - ETA: 0s - loss: 9.1826 - dense_3_loss: 2.2549 - dense_3_accuracy: 0.9626 - dense_3_accuracy_1: 0.9619 - dense_3_accuracy_2: 0.5068 - dense_3_accuracy_3: 0.1600 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.8983 - dense_3_accuracy_6: 0.2487 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.4456 - dense_3_accuracy_9: 0.15 - ETA: 0s - loss: 9.1824 - dense_3_loss: 2.2559 - dense_3_accuracy: 0.9615 - dense_3_accuracy_1: 0.9607 - dense_3_accuracy_2: 0.5038 - dense_3_accuracy_3: 0.1611 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.8995 - dense_3_accuracy_6: 0.2579 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.4463 - dense_3_accuracy_9: 0.15 - 1s 101us/step - loss: 9.1676 - dense_3_loss: 2.2590 - dense_3_accuracy: 0.9620 - dense_3_accuracy_1: 0.9616 - dense_3_accuracy_2: 0.5071 - dense_3_accuracy_3: 0.1629 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9010 - dense_3_accuracy_6: 0.2623 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.4459 - dense_3_accuracy_9: 0.1540 - val_loss: 9.1832 - val_dense_3_loss: 2.2481 - val_dense_3_accuracy: 0.9470 - val_dense_3_accuracy_1: 0.9480 - val_dense_3_accuracy_2: 0.4890 - val_dense_3_accuracy_3: 0.1520 - val_dense_3_accuracy_4: 1.0000 - val_dense_3_accuracy_5: 0.9290 - val_dense_3_accuracy_6: 0.3100 - val_dense_3_accuracy_7: 1.0000 - val_dense_3_accuracy_8: 0.4310 - val_dense_3_accuracy_9: 0.1590\n",
      "Epoch 19/40\n",
      "9000/9000 [==============================] - ETA: 0s - loss: 9.1115 - dense_3_loss: 2.2597 - dense_3_accuracy: 0.9614 - dense_3_accuracy_1: 0.9609 - dense_3_accuracy_2: 0.5146 - dense_3_accuracy_3: 0.1416 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9126 - dense_3_accuracy_6: 0.3174 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.4453 - dense_3_accuracy_9: 0.16 - ETA: 0s - loss: 9.0580 - dense_3_loss: 2.2582 - dense_3_accuracy: 0.9661 - dense_3_accuracy_1: 0.9651 - dense_3_accuracy_2: 0.5266 - dense_3_accuracy_3: 0.1494 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9133 - dense_3_accuracy_6: 0.3171 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.4543 - dense_3_accuracy_9: 0.16 - ETA: 0s - loss: 9.0296 - dense_3_loss: 2.2532 - dense_3_accuracy: 0.9673 - dense_3_accuracy_1: 0.9653 - dense_3_accuracy_2: 0.5166 - dense_3_accuracy_3: 0.1592 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9191 - dense_3_accuracy_6: 0.3148 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.4486 - dense_3_accuracy_9: 0.17 - ETA: 0s - loss: 9.0204 - dense_3_loss: 2.2492 - dense_3_accuracy: 0.9662 - dense_3_accuracy_1: 0.9645 - dense_3_accuracy_2: 0.5138 - dense_3_accuracy_3: 0.1637 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9178 - dense_3_accuracy_6: 0.3259 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.4482 - dense_3_accuracy_9: 0.17 - 1s 101us/step - loss: 9.0277 - dense_3_loss: 2.2526 - dense_3_accuracy: 0.9657 - dense_3_accuracy_1: 0.9639 - dense_3_accuracy_2: 0.5134 - dense_3_accuracy_3: 0.1607 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9167 - dense_3_accuracy_6: 0.3260 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.4496 - dense_3_accuracy_9: 0.1704 - val_loss: 8.9614 - val_dense_3_loss: 2.2395 - val_dense_3_accuracy: 0.9620 - val_dense_3_accuracy_1: 0.9620 - val_dense_3_accuracy_2: 0.5100 - val_dense_3_accuracy_3: 0.1670 - val_dense_3_accuracy_4: 1.0000 - val_dense_3_accuracy_5: 0.9420 - val_dense_3_accuracy_6: 0.3170 - val_dense_3_accuracy_7: 1.0000 - val_dense_3_accuracy_8: 0.4570 - val_dense_3_accuracy_9: 0.1660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/40\n",
      "9000/9000 [==============================] - ETA: 0s - loss: 8.8591 - dense_3_loss: 2.2298 - dense_3_accuracy: 0.9741 - dense_3_accuracy_1: 0.9741 - dense_3_accuracy_2: 0.5234 - dense_3_accuracy_3: 0.1689 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9160 - dense_3_accuracy_6: 0.3340 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.4663 - dense_3_accuracy_9: 0.16 - ETA: 0s - loss: 8.8773 - dense_3_loss: 2.2291 - dense_3_accuracy: 0.9670 - dense_3_accuracy_1: 0.9670 - dense_3_accuracy_2: 0.5293 - dense_3_accuracy_3: 0.1785 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9272 - dense_3_accuracy_6: 0.3350 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.4739 - dense_3_accuracy_9: 0.17 - ETA: 0s - loss: 8.8707 - dense_3_loss: 2.2345 - dense_3_accuracy: 0.9686 - dense_3_accuracy_1: 0.9684 - dense_3_accuracy_2: 0.5350 - dense_3_accuracy_3: 0.1740 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9268 - dense_3_accuracy_6: 0.3358 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.4753 - dense_3_accuracy_9: 0.17 - ETA: 0s - loss: 8.8755 - dense_3_loss: 2.2360 - dense_3_accuracy: 0.9685 - dense_3_accuracy_1: 0.9681 - dense_3_accuracy_2: 0.5331 - dense_3_accuracy_3: 0.1779 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9237 - dense_3_accuracy_6: 0.3373 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.4650 - dense_3_accuracy_9: 0.17 - 1s 103us/step - loss: 8.8738 - dense_3_loss: 2.2393 - dense_3_accuracy: 0.9689 - dense_3_accuracy_1: 0.9686 - dense_3_accuracy_2: 0.5328 - dense_3_accuracy_3: 0.1749 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9242 - dense_3_accuracy_6: 0.3384 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.4670 - dense_3_accuracy_9: 0.1732 - val_loss: 8.7459 - val_dense_3_loss: 2.2205 - val_dense_3_accuracy: 0.9680 - val_dense_3_accuracy_1: 0.9670 - val_dense_3_accuracy_2: 0.5380 - val_dense_3_accuracy_3: 0.2020 - val_dense_3_accuracy_4: 1.0000 - val_dense_3_accuracy_5: 0.9550 - val_dense_3_accuracy_6: 0.3520 - val_dense_3_accuracy_7: 1.0000 - val_dense_3_accuracy_8: 0.4840 - val_dense_3_accuracy_9: 0.1710\n",
      "Epoch 21/40\n",
      "9000/9000 [==============================] - ETA: 0s - loss: 8.6585 - dense_3_loss: 2.2196 - dense_3_accuracy: 0.9736 - dense_3_accuracy_1: 0.9736 - dense_3_accuracy_2: 0.5522 - dense_3_accuracy_3: 0.1963 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9487 - dense_3_accuracy_6: 0.3662 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.4849 - dense_3_accuracy_9: 0.19 - ETA: 0s - loss: 8.6794 - dense_3_loss: 2.2201 - dense_3_accuracy: 0.9722 - dense_3_accuracy_1: 0.9719 - dense_3_accuracy_2: 0.5466 - dense_3_accuracy_3: 0.1938 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9448 - dense_3_accuracy_6: 0.3613 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.4924 - dense_3_accuracy_9: 0.18 - ETA: 0s - loss: 8.6526 - dense_3_loss: 2.2194 - dense_3_accuracy: 0.9720 - dense_3_accuracy_1: 0.9714 - dense_3_accuracy_2: 0.5487 - dense_3_accuracy_3: 0.1958 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9442 - dense_3_accuracy_6: 0.3774 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.4967 - dense_3_accuracy_9: 0.18 - ETA: 0s - loss: 8.6435 - dense_3_loss: 2.2152 - dense_3_accuracy: 0.9707 - dense_3_accuracy_1: 0.9698 - dense_3_accuracy_2: 0.5486 - dense_3_accuracy_3: 0.1947 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9436 - dense_3_accuracy_6: 0.3866 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.4991 - dense_3_accuracy_9: 0.18 - 1s 100us/step - loss: 8.6360 - dense_3_loss: 2.2119 - dense_3_accuracy: 0.9711 - dense_3_accuracy_1: 0.9700 - dense_3_accuracy_2: 0.5497 - dense_3_accuracy_3: 0.1963 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9429 - dense_3_accuracy_6: 0.3912 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.4978 - dense_3_accuracy_9: 0.1881 - val_loss: 8.5436 - val_dense_3_loss: 2.1986 - val_dense_3_accuracy: 0.9700 - val_dense_3_accuracy_1: 0.9670 - val_dense_3_accuracy_2: 0.5510 - val_dense_3_accuracy_3: 0.2130 - val_dense_3_accuracy_4: 1.0000 - val_dense_3_accuracy_5: 0.9630 - val_dense_3_accuracy_6: 0.3880 - val_dense_3_accuracy_7: 1.0000 - val_dense_3_accuracy_8: 0.4740 - val_dense_3_accuracy_9: 0.1850\n",
      "Epoch 22/40\n",
      "9000/9000 [==============================] - ETA: 0s - loss: 8.4572 - dense_3_loss: 2.1933 - dense_3_accuracy: 0.9712 - dense_3_accuracy_1: 0.9712 - dense_3_accuracy_2: 0.5664 - dense_3_accuracy_3: 0.1929 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9482 - dense_3_accuracy_6: 0.4355 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.4927 - dense_3_accuracy_9: 0.19 - ETA: 0s - loss: 8.4525 - dense_3_loss: 2.2017 - dense_3_accuracy: 0.9719 - dense_3_accuracy_1: 0.9722 - dense_3_accuracy_2: 0.5593 - dense_3_accuracy_3: 0.1951 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9517 - dense_3_accuracy_6: 0.4348 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.5056 - dense_3_accuracy_9: 0.18 - ETA: 0s - loss: 8.4454 - dense_3_loss: 2.1948 - dense_3_accuracy: 0.9715 - dense_3_accuracy_1: 0.9710 - dense_3_accuracy_2: 0.5524 - dense_3_accuracy_3: 0.2010 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9502 - dense_3_accuracy_6: 0.4277 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.5037 - dense_3_accuracy_9: 0.19 - ETA: 0s - loss: 8.4338 - dense_3_loss: 2.1895 - dense_3_accuracy: 0.9714 - dense_3_accuracy_1: 0.9708 - dense_3_accuracy_2: 0.5486 - dense_3_accuracy_3: 0.2009 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9491 - dense_3_accuracy_6: 0.4235 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.5032 - dense_3_accuracy_9: 0.19 - 1s 100us/step - loss: 8.4296 - dense_3_loss: 2.1879 - dense_3_accuracy: 0.9718 - dense_3_accuracy_1: 0.9710 - dense_3_accuracy_2: 0.5504 - dense_3_accuracy_3: 0.2008 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9482 - dense_3_accuracy_6: 0.4240 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.5044 - dense_3_accuracy_9: 0.1926 - val_loss: 8.3787 - val_dense_3_loss: 2.1731 - val_dense_3_accuracy: 0.9670 - val_dense_3_accuracy_1: 0.9640 - val_dense_3_accuracy_2: 0.5450 - val_dense_3_accuracy_3: 0.2180 - val_dense_3_accuracy_4: 1.0000 - val_dense_3_accuracy_5: 0.9670 - val_dense_3_accuracy_6: 0.4400 - val_dense_3_accuracy_7: 1.0000 - val_dense_3_accuracy_8: 0.4880 - val_dense_3_accuracy_9: 0.1990\n",
      "Epoch 23/40\n",
      "9000/9000 [==============================] - ETA: 0s - loss: 8.2712 - dense_3_loss: 2.1652 - dense_3_accuracy: 0.9702 - dense_3_accuracy_1: 0.9707 - dense_3_accuracy_2: 0.5596 - dense_3_accuracy_3: 0.2256 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9473 - dense_3_accuracy_6: 0.4453 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.5200 - dense_3_accuracy_9: 0.21 - ETA: 0s - loss: 8.3204 - dense_3_loss: 2.1651 - dense_3_accuracy: 0.9697 - dense_3_accuracy_1: 0.9712 - dense_3_accuracy_2: 0.5654 - dense_3_accuracy_3: 0.2061 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9497 - dense_3_accuracy_6: 0.4517 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.5107 - dense_3_accuracy_9: 0.21 - ETA: 0s - loss: 8.3501 - dense_3_loss: 2.1715 - dense_3_accuracy: 0.9710 - dense_3_accuracy_1: 0.9733 - dense_3_accuracy_2: 0.5656 - dense_3_accuracy_3: 0.2077 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9386 - dense_3_accuracy_6: 0.4354 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.5135 - dense_3_accuracy_9: 0.20 - ETA: 0s - loss: 8.3546 - dense_3_loss: 2.1693 - dense_3_accuracy: 0.9702 - dense_3_accuracy_1: 0.9720 - dense_3_accuracy_2: 0.5651 - dense_3_accuracy_3: 0.2051 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9382 - dense_3_accuracy_6: 0.4399 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.5103 - dense_3_accuracy_9: 0.20 - 1s 99us/step - loss: 8.3375 - dense_3_loss: 2.1625 - dense_3_accuracy: 0.9704 - dense_3_accuracy_1: 0.9722 - dense_3_accuracy_2: 0.5642 - dense_3_accuracy_3: 0.2056 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9404 - dense_3_accuracy_6: 0.4450 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.5116 - dense_3_accuracy_9: 0.2058 - val_loss: 8.3612 - val_dense_3_loss: 2.1707 - val_dense_3_accuracy: 0.9730 - val_dense_3_accuracy_1: 0.9690 - val_dense_3_accuracy_2: 0.5750 - val_dense_3_accuracy_3: 0.2220 - val_dense_3_accuracy_4: 1.0000 - val_dense_3_accuracy_5: 0.9340 - val_dense_3_accuracy_6: 0.4130 - val_dense_3_accuracy_7: 1.0000 - val_dense_3_accuracy_8: 0.4880 - val_dense_3_accuracy_9: 0.2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/40\n",
      "9000/9000 [==============================] - ETA: 0s - loss: 8.2966 - dense_3_loss: 2.1712 - dense_3_accuracy: 0.9702 - dense_3_accuracy_1: 0.9707 - dense_3_accuracy_2: 0.5835 - dense_3_accuracy_3: 0.2104 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9282 - dense_3_accuracy_6: 0.4360 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.5132 - dense_3_accuracy_9: 0.20 - ETA: 0s - loss: 8.1864 - dense_3_loss: 2.1486 - dense_3_accuracy: 0.9749 - dense_3_accuracy_1: 0.9753 - dense_3_accuracy_2: 0.5737 - dense_3_accuracy_3: 0.2197 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9463 - dense_3_accuracy_6: 0.4575 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.5144 - dense_3_accuracy_9: 0.22 - ETA: 0s - loss: 8.1802 - dense_3_loss: 2.1440 - dense_3_accuracy: 0.9725 - dense_3_accuracy_1: 0.9717 - dense_3_accuracy_2: 0.5767 - dense_3_accuracy_3: 0.2183 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9486 - dense_3_accuracy_6: 0.4569 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.5173 - dense_3_accuracy_9: 0.22 - ETA: 0s - loss: 8.1348 - dense_3_loss: 2.1435 - dense_3_accuracy: 0.9736 - dense_3_accuracy_1: 0.9729 - dense_3_accuracy_2: 0.5753 - dense_3_accuracy_3: 0.2184 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9532 - dense_3_accuracy_6: 0.4678 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.5292 - dense_3_accuracy_9: 0.21 - 1s 100us/step - loss: 8.1351 - dense_3_loss: 2.1461 - dense_3_accuracy: 0.9737 - dense_3_accuracy_1: 0.9729 - dense_3_accuracy_2: 0.5706 - dense_3_accuracy_3: 0.2199 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9538 - dense_3_accuracy_6: 0.4669 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.5313 - dense_3_accuracy_9: 0.2133 - val_loss: 8.0215 - val_dense_3_loss: 2.1165 - val_dense_3_accuracy: 0.9720 - val_dense_3_accuracy_1: 0.9740 - val_dense_3_accuracy_2: 0.5570 - val_dense_3_accuracy_3: 0.2220 - val_dense_3_accuracy_4: 1.0000 - val_dense_3_accuracy_5: 0.9740 - val_dense_3_accuracy_6: 0.5110 - val_dense_3_accuracy_7: 1.0000 - val_dense_3_accuracy_8: 0.5330 - val_dense_3_accuracy_9: 0.2160\n",
      "Epoch 25/40\n",
      "9000/9000 [==============================] - ETA: 0s - loss: 7.9280 - dense_3_loss: 2.1165 - dense_3_accuracy: 0.9795 - dense_3_accuracy_1: 0.9790 - dense_3_accuracy_2: 0.5869 - dense_3_accuracy_3: 0.2100 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9663 - dense_3_accuracy_6: 0.5181 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.5508 - dense_3_accuracy_9: 0.23 - ETA: 0s - loss: 7.9337 - dense_3_loss: 2.1181 - dense_3_accuracy: 0.9771 - dense_3_accuracy_1: 0.9761 - dense_3_accuracy_2: 0.5867 - dense_3_accuracy_3: 0.2234 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9622 - dense_3_accuracy_6: 0.5051 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.5498 - dense_3_accuracy_9: 0.22 - ETA: 0s - loss: 7.9081 - dense_3_loss: 2.1119 - dense_3_accuracy: 0.9771 - dense_3_accuracy_1: 0.9766 - dense_3_accuracy_2: 0.5892 - dense_3_accuracy_3: 0.2269 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9642 - dense_3_accuracy_6: 0.5085 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.5573 - dense_3_accuracy_9: 0.22 - ETA: 0s - loss: 7.8999 - dense_3_loss: 2.1045 - dense_3_accuracy: 0.9763 - dense_3_accuracy_1: 0.9764 - dense_3_accuracy_2: 0.5876 - dense_3_accuracy_3: 0.2263 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9635 - dense_3_accuracy_6: 0.5050 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.5538 - dense_3_accuracy_9: 0.22 - 1s 101us/step - loss: 7.8996 - dense_3_loss: 2.1056 - dense_3_accuracy: 0.9753 - dense_3_accuracy_1: 0.9753 - dense_3_accuracy_2: 0.5862 - dense_3_accuracy_3: 0.2267 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9632 - dense_3_accuracy_6: 0.5073 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.5563 - dense_3_accuracy_9: 0.2292 - val_loss: 7.8391 - val_dense_3_loss: 2.0901 - val_dense_3_accuracy: 0.9750 - val_dense_3_accuracy_1: 0.9740 - val_dense_3_accuracy_2: 0.5930 - val_dense_3_accuracy_3: 0.2440 - val_dense_3_accuracy_4: 1.0000 - val_dense_3_accuracy_5: 0.9760 - val_dense_3_accuracy_6: 0.5380 - val_dense_3_accuracy_7: 1.0000 - val_dense_3_accuracy_8: 0.5210 - val_dense_3_accuracy_9: 0.2300\n",
      "Epoch 26/40\n",
      "9000/9000 [==============================] - ETA: 0s - loss: 7.7104 - dense_3_loss: 2.0909 - dense_3_accuracy: 0.9785 - dense_3_accuracy_1: 0.9785 - dense_3_accuracy_2: 0.5854 - dense_3_accuracy_3: 0.2280 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9673 - dense_3_accuracy_6: 0.5342 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.5537 - dense_3_accuracy_9: 0.23 - ETA: 0s - loss: 7.7432 - dense_3_loss: 2.0803 - dense_3_accuracy: 0.9800 - dense_3_accuracy_1: 0.9795 - dense_3_accuracy_2: 0.5898 - dense_3_accuracy_3: 0.2302 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9656 - dense_3_accuracy_6: 0.5295 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.5557 - dense_3_accuracy_9: 0.22 - ETA: 0s - loss: 7.7176 - dense_3_loss: 2.0820 - dense_3_accuracy: 0.9780 - dense_3_accuracy_1: 0.9774 - dense_3_accuracy_2: 0.5934 - dense_3_accuracy_3: 0.2316 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9674 - dense_3_accuracy_6: 0.5402 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.5558 - dense_3_accuracy_9: 0.22 - ETA: 0s - loss: 7.7137 - dense_3_loss: 2.0777 - dense_3_accuracy: 0.9766 - dense_3_accuracy_1: 0.9756 - dense_3_accuracy_2: 0.5925 - dense_3_accuracy_3: 0.2338 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9672 - dense_3_accuracy_6: 0.5359 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.5626 - dense_3_accuracy_9: 0.23 - 1s 99us/step - loss: 7.7033 - dense_3_loss: 2.0710 - dense_3_accuracy: 0.9758 - dense_3_accuracy_1: 0.9751 - dense_3_accuracy_2: 0.5951 - dense_3_accuracy_3: 0.2353 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9680 - dense_3_accuracy_6: 0.5370 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.5624 - dense_3_accuracy_9: 0.2307 - val_loss: 7.6427 - val_dense_3_loss: 2.0463 - val_dense_3_accuracy: 0.9750 - val_dense_3_accuracy_1: 0.9740 - val_dense_3_accuracy_2: 0.5900 - val_dense_3_accuracy_3: 0.2510 - val_dense_3_accuracy_4: 1.0000 - val_dense_3_accuracy_5: 0.9790 - val_dense_3_accuracy_6: 0.5530 - val_dense_3_accuracy_7: 1.0000 - val_dense_3_accuracy_8: 0.5570 - val_dense_3_accuracy_9: 0.2500\n",
      "Epoch 27/40\n",
      "9000/9000 [==============================] - ETA: 0s - loss: 7.6100 - dense_3_loss: 2.0547 - dense_3_accuracy: 0.9751 - dense_3_accuracy_1: 0.9756 - dense_3_accuracy_2: 0.5830 - dense_3_accuracy_3: 0.2373 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9634 - dense_3_accuracy_6: 0.5542 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.5879 - dense_3_accuracy_9: 0.23 - ETA: 0s - loss: 7.5646 - dense_3_loss: 2.0357 - dense_3_accuracy: 0.9761 - dense_3_accuracy_1: 0.9768 - dense_3_accuracy_2: 0.5847 - dense_3_accuracy_3: 0.2498 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9668 - dense_3_accuracy_6: 0.5593 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.5735 - dense_3_accuracy_9: 0.25 - ETA: 0s - loss: 7.5448 - dense_3_loss: 2.0377 - dense_3_accuracy: 0.9766 - dense_3_accuracy_1: 0.9772 - dense_3_accuracy_2: 0.5990 - dense_3_accuracy_3: 0.2505 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9678 - dense_3_accuracy_6: 0.5605 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.5770 - dense_3_accuracy_9: 0.24 - ETA: 0s - loss: 7.5220 - dense_3_loss: 2.0360 - dense_3_accuracy: 0.9755 - dense_3_accuracy_1: 0.9762 - dense_3_accuracy_2: 0.6039 - dense_3_accuracy_3: 0.2488 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9679 - dense_3_accuracy_6: 0.5654 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.5854 - dense_3_accuracy_9: 0.24 - 1s 99us/step - loss: 7.5146 - dense_3_loss: 2.0271 - dense_3_accuracy: 0.9758 - dense_3_accuracy_1: 0.9760 - dense_3_accuracy_2: 0.6064 - dense_3_accuracy_3: 0.2493 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9687 - dense_3_accuracy_6: 0.5683 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.5834 - dense_3_accuracy_9: 0.2469 - val_loss: 7.4539 - val_dense_3_loss: 2.0110 - val_dense_3_accuracy: 0.9720 - val_dense_3_accuracy_1: 0.9710 - val_dense_3_accuracy_2: 0.6090 - val_dense_3_accuracy_3: 0.2540 - val_dense_3_accuracy_4: 1.0000 - val_dense_3_accuracy_5: 0.9840 - val_dense_3_accuracy_6: 0.5880 - val_dense_3_accuracy_7: 1.0000 - val_dense_3_accuracy_8: 0.5750 - val_dense_3_accuracy_9: 0.2420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/40\n",
      "9000/9000 [==============================] - ETA: 0s - loss: 7.4190 - dense_3_loss: 2.0264 - dense_3_accuracy: 0.9805 - dense_3_accuracy_1: 0.9800 - dense_3_accuracy_2: 0.6191 - dense_3_accuracy_3: 0.2578 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9697 - dense_3_accuracy_6: 0.5752 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.5898 - dense_3_accuracy_9: 0.24 - ETA: 0s - loss: 7.3934 - dense_3_loss: 2.0206 - dense_3_accuracy: 0.9788 - dense_3_accuracy_1: 0.9792 - dense_3_accuracy_2: 0.6206 - dense_3_accuracy_3: 0.2595 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9707 - dense_3_accuracy_6: 0.5762 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.5947 - dense_3_accuracy_9: 0.24 - ETA: 0s - loss: 7.3967 - dense_3_loss: 2.0120 - dense_3_accuracy: 0.9764 - dense_3_accuracy_1: 0.9772 - dense_3_accuracy_2: 0.6143 - dense_3_accuracy_3: 0.2604 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9707 - dense_3_accuracy_6: 0.5747 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.5861 - dense_3_accuracy_9: 0.25 - ETA: 0s - loss: 7.3646 - dense_3_loss: 2.0045 - dense_3_accuracy: 0.9772 - dense_3_accuracy_1: 0.9781 - dense_3_accuracy_2: 0.6139 - dense_3_accuracy_3: 0.2637 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9703 - dense_3_accuracy_6: 0.5795 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.5916 - dense_3_accuracy_9: 0.25 - 1s 99us/step - loss: 7.3549 - dense_3_loss: 1.9947 - dense_3_accuracy: 0.9766 - dense_3_accuracy_1: 0.9778 - dense_3_accuracy_2: 0.6152 - dense_3_accuracy_3: 0.2614 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9712 - dense_3_accuracy_6: 0.5826 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.5922 - dense_3_accuracy_9: 0.2583 - val_loss: 7.2581 - val_dense_3_loss: 1.9520 - val_dense_3_accuracy: 0.9710 - val_dense_3_accuracy_1: 0.9710 - val_dense_3_accuracy_2: 0.6150 - val_dense_3_accuracy_3: 0.2750 - val_dense_3_accuracy_4: 1.0000 - val_dense_3_accuracy_5: 0.9810 - val_dense_3_accuracy_6: 0.6110 - val_dense_3_accuracy_7: 1.0000 - val_dense_3_accuracy_8: 0.5970 - val_dense_3_accuracy_9: 0.2850\n",
      "Epoch 29/40\n",
      "9000/9000 [==============================] - ETA: 0s - loss: 7.2353 - dense_3_loss: 1.9541 - dense_3_accuracy: 0.9761 - dense_3_accuracy_1: 0.9756 - dense_3_accuracy_2: 0.6079 - dense_3_accuracy_3: 0.2627 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9688 - dense_3_accuracy_6: 0.5991 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.6206 - dense_3_accuracy_9: 0.27 - ETA: 0s - loss: 7.1782 - dense_3_loss: 1.9398 - dense_3_accuracy: 0.9788 - dense_3_accuracy_1: 0.9783 - dense_3_accuracy_2: 0.6152 - dense_3_accuracy_3: 0.2727 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9724 - dense_3_accuracy_6: 0.6038 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.6116 - dense_3_accuracy_9: 0.28 - ETA: 0s - loss: 7.1819 - dense_3_loss: 1.9528 - dense_3_accuracy: 0.9790 - dense_3_accuracy_1: 0.9784 - dense_3_accuracy_2: 0.6206 - dense_3_accuracy_3: 0.2734 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9731 - dense_3_accuracy_6: 0.6038 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.6045 - dense_3_accuracy_9: 0.27 - ETA: 0s - loss: 7.1805 - dense_3_loss: 1.9523 - dense_3_accuracy: 0.9763 - dense_3_accuracy_1: 0.9760 - dense_3_accuracy_2: 0.6193 - dense_3_accuracy_3: 0.2733 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9722 - dense_3_accuracy_6: 0.6046 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.6058 - dense_3_accuracy_9: 0.26 - 1s 101us/step - loss: 7.1740 - dense_3_loss: 1.9473 - dense_3_accuracy: 0.9768 - dense_3_accuracy_1: 0.9766 - dense_3_accuracy_2: 0.6189 - dense_3_accuracy_3: 0.2710 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9719 - dense_3_accuracy_6: 0.6062 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.6086 - dense_3_accuracy_9: 0.2707 - val_loss: 7.1007 - val_dense_3_loss: 1.9167 - val_dense_3_accuracy: 0.9680 - val_dense_3_accuracy_1: 0.9670 - val_dense_3_accuracy_2: 0.6160 - val_dense_3_accuracy_3: 0.2720 - val_dense_3_accuracy_4: 1.0000 - val_dense_3_accuracy_5: 0.9840 - val_dense_3_accuracy_6: 0.6360 - val_dense_3_accuracy_7: 1.0000 - val_dense_3_accuracy_8: 0.6210 - val_dense_3_accuracy_9: 0.2720\n",
      "Epoch 30/40\n",
      "9000/9000 [==============================] - ETA: 0s - loss: 7.1193 - dense_3_loss: 1.9302 - dense_3_accuracy: 0.9722 - dense_3_accuracy_1: 0.9741 - dense_3_accuracy_2: 0.6177 - dense_3_accuracy_3: 0.2837 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9688 - dense_3_accuracy_6: 0.6074 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.6328 - dense_3_accuracy_9: 0.27 - ETA: 0s - loss: 7.0767 - dense_3_loss: 1.9107 - dense_3_accuracy: 0.9731 - dense_3_accuracy_1: 0.9741 - dense_3_accuracy_2: 0.6025 - dense_3_accuracy_3: 0.2744 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9712 - dense_3_accuracy_6: 0.6216 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.6157 - dense_3_accuracy_9: 0.27 - ETA: 0s - loss: 7.0757 - dense_3_loss: 1.9067 - dense_3_accuracy: 0.9735 - dense_3_accuracy_1: 0.9731 - dense_3_accuracy_2: 0.6112 - dense_3_accuracy_3: 0.2723 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9701 - dense_3_accuracy_6: 0.6258 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.6082 - dense_3_accuracy_9: 0.28 - ETA: 0s - loss: 7.0657 - dense_3_loss: 1.9139 - dense_3_accuracy: 0.9745 - dense_3_accuracy_1: 0.9742 - dense_3_accuracy_2: 0.6210 - dense_3_accuracy_3: 0.2753 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9691 - dense_3_accuracy_6: 0.6178 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.6102 - dense_3_accuracy_9: 0.28 - 1s 98us/step - loss: 7.0668 - dense_3_loss: 1.9238 - dense_3_accuracy: 0.9750 - dense_3_accuracy_1: 0.9747 - dense_3_accuracy_2: 0.6230 - dense_3_accuracy_3: 0.2759 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9692 - dense_3_accuracy_6: 0.6182 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.6087 - dense_3_accuracy_9: 0.2840 - val_loss: 7.0833 - val_dense_3_loss: 1.9467 - val_dense_3_accuracy: 0.9760 - val_dense_3_accuracy_1: 0.9770 - val_dense_3_accuracy_2: 0.6150 - val_dense_3_accuracy_3: 0.2700 - val_dense_3_accuracy_4: 1.0000 - val_dense_3_accuracy_5: 0.9830 - val_dense_3_accuracy_6: 0.6220 - val_dense_3_accuracy_7: 1.0000 - val_dense_3_accuracy_8: 0.6170 - val_dense_3_accuracy_9: 0.2790\n",
      "Epoch 31/40\n",
      "9000/9000 [==============================] - ETA: 0s - loss: 7.0072 - dense_3_loss: 1.9328 - dense_3_accuracy: 0.9810 - dense_3_accuracy_1: 0.9805 - dense_3_accuracy_2: 0.6353 - dense_3_accuracy_3: 0.2690 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9756 - dense_3_accuracy_6: 0.6050 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.6333 - dense_3_accuracy_9: 0.28 - ETA: 0s - loss: 6.9810 - dense_3_loss: 1.9109 - dense_3_accuracy: 0.9773 - dense_3_accuracy_1: 0.9758 - dense_3_accuracy_2: 0.6343 - dense_3_accuracy_3: 0.2830 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9751 - dense_3_accuracy_6: 0.6125 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.6311 - dense_3_accuracy_9: 0.29 - ETA: 0s - loss: 6.9505 - dense_3_loss: 1.9026 - dense_3_accuracy: 0.9771 - dense_3_accuracy_1: 0.9754 - dense_3_accuracy_2: 0.6405 - dense_3_accuracy_3: 0.2837 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9735 - dense_3_accuracy_6: 0.6260 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.6265 - dense_3_accuracy_9: 0.29 - ETA: 0s - loss: 6.9338 - dense_3_loss: 1.9005 - dense_3_accuracy: 0.9775 - dense_3_accuracy_1: 0.9760 - dense_3_accuracy_2: 0.6407 - dense_3_accuracy_3: 0.2815 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9729 - dense_3_accuracy_6: 0.6244 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.6301 - dense_3_accuracy_9: 0.29 - 1s 100us/step - loss: 6.9198 - dense_3_loss: 1.8934 - dense_3_accuracy: 0.9771 - dense_3_accuracy_1: 0.9757 - dense_3_accuracy_2: 0.6428 - dense_3_accuracy_3: 0.2838 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9724 - dense_3_accuracy_6: 0.6256 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.6317 - dense_3_accuracy_9: 0.2906 - val_loss: 6.7804 - val_dense_3_loss: 1.8575 - val_dense_3_accuracy: 0.9760 - val_dense_3_accuracy_1: 0.9770 - val_dense_3_accuracy_2: 0.6570 - val_dense_3_accuracy_3: 0.3010 - val_dense_3_accuracy_4: 1.0000 - val_dense_3_accuracy_5: 0.9820 - val_dense_3_accuracy_6: 0.6470 - val_dense_3_accuracy_7: 1.0000 - val_dense_3_accuracy_8: 0.6240 - val_dense_3_accuracy_9: 0.3080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/40\n",
      "9000/9000 [==============================] - ETA: 0s - loss: 6.6889 - dense_3_loss: 1.8601 - dense_3_accuracy: 0.9824 - dense_3_accuracy_1: 0.9810 - dense_3_accuracy_2: 0.6528 - dense_3_accuracy_3: 0.3232 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9736 - dense_3_accuracy_6: 0.6484 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.6597 - dense_3_accuracy_9: 0.30 - ETA: 0s - loss: 6.7377 - dense_3_loss: 1.8473 - dense_3_accuracy: 0.9788 - dense_3_accuracy_1: 0.9780 - dense_3_accuracy_2: 0.6587 - dense_3_accuracy_3: 0.3044 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9766 - dense_3_accuracy_6: 0.6326 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.6433 - dense_3_accuracy_9: 0.31 - ETA: 0s - loss: 6.7446 - dense_3_loss: 1.8516 - dense_3_accuracy: 0.9788 - dense_3_accuracy_1: 0.9784 - dense_3_accuracy_2: 0.6608 - dense_3_accuracy_3: 0.3047 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9736 - dense_3_accuracy_6: 0.6312 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.6419 - dense_3_accuracy_9: 0.30 - ETA: 0s - loss: 6.7177 - dense_3_loss: 1.8440 - dense_3_accuracy: 0.9777 - dense_3_accuracy_1: 0.9774 - dense_3_accuracy_2: 0.6656 - dense_3_accuracy_3: 0.3090 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9730 - dense_3_accuracy_6: 0.6304 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.6486 - dense_3_accuracy_9: 0.30 - 1s 100us/step - loss: 6.7248 - dense_3_loss: 1.8461 - dense_3_accuracy: 0.9770 - dense_3_accuracy_1: 0.9767 - dense_3_accuracy_2: 0.6673 - dense_3_accuracy_3: 0.3079 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9728 - dense_3_accuracy_6: 0.6308 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.6468 - dense_3_accuracy_9: 0.3091 - val_loss: 6.7160 - val_dense_3_loss: 1.8402 - val_dense_3_accuracy: 0.9770 - val_dense_3_accuracy_1: 0.9770 - val_dense_3_accuracy_2: 0.6700 - val_dense_3_accuracy_3: 0.3110 - val_dense_3_accuracy_4: 1.0000 - val_dense_3_accuracy_5: 0.9790 - val_dense_3_accuracy_6: 0.6500 - val_dense_3_accuracy_7: 1.0000 - val_dense_3_accuracy_8: 0.5840 - val_dense_3_accuracy_9: 0.3190\n",
      "Epoch 33/40\n",
      "9000/9000 [==============================] - ETA: 0s - loss: 6.6608 - dense_3_loss: 1.8284 - dense_3_accuracy: 0.9775 - dense_3_accuracy_1: 0.9756 - dense_3_accuracy_2: 0.6909 - dense_3_accuracy_3: 0.3120 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9751 - dense_3_accuracy_6: 0.6304 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.6187 - dense_3_accuracy_9: 0.31 - ETA: 0s - loss: 6.6122 - dense_3_loss: 1.8075 - dense_3_accuracy: 0.9780 - dense_3_accuracy_1: 0.9756 - dense_3_accuracy_2: 0.6868 - dense_3_accuracy_3: 0.3220 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9731 - dense_3_accuracy_6: 0.6428 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.6265 - dense_3_accuracy_9: 0.31 - ETA: 0s - loss: 6.5773 - dense_3_loss: 1.7984 - dense_3_accuracy: 0.9784 - dense_3_accuracy_1: 0.9764 - dense_3_accuracy_2: 0.6899 - dense_3_accuracy_3: 0.3309 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9725 - dense_3_accuracy_6: 0.6449 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.6318 - dense_3_accuracy_9: 0.32 - ETA: 0s - loss: 6.5514 - dense_3_loss: 1.7981 - dense_3_accuracy: 0.9786 - dense_3_accuracy_1: 0.9783 - dense_3_accuracy_2: 0.6930 - dense_3_accuracy_3: 0.3315 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9718 - dense_3_accuracy_6: 0.6500 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.6385 - dense_3_accuracy_9: 0.32 - 1s 100us/step - loss: 6.5487 - dense_3_loss: 1.8021 - dense_3_accuracy: 0.9778 - dense_3_accuracy_1: 0.9781 - dense_3_accuracy_2: 0.6926 - dense_3_accuracy_3: 0.3292 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9717 - dense_3_accuracy_6: 0.6487 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.6410 - dense_3_accuracy_9: 0.3192 - val_loss: 6.3934 - val_dense_3_loss: 1.7179 - val_dense_3_accuracy: 0.9790 - val_dense_3_accuracy_1: 0.9770 - val_dense_3_accuracy_2: 0.6860 - val_dense_3_accuracy_3: 0.3040 - val_dense_3_accuracy_4: 1.0000 - val_dense_3_accuracy_5: 0.9810 - val_dense_3_accuracy_6: 0.6690 - val_dense_3_accuracy_7: 1.0000 - val_dense_3_accuracy_8: 0.6240 - val_dense_3_accuracy_9: 0.3560\n",
      "Epoch 34/40\n",
      "9000/9000 [==============================] - ETA: 0s - loss: 6.3664 - dense_3_loss: 1.7088 - dense_3_accuracy: 0.9819 - dense_3_accuracy_1: 0.9819 - dense_3_accuracy_2: 0.6963 - dense_3_accuracy_3: 0.3071 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9751 - dense_3_accuracy_6: 0.6426 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.6606 - dense_3_accuracy_9: 0.34 - ETA: 0s - loss: 6.3563 - dense_3_loss: 1.7177 - dense_3_accuracy: 0.9783 - dense_3_accuracy_1: 0.9783 - dense_3_accuracy_2: 0.6987 - dense_3_accuracy_3: 0.3291 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9751 - dense_3_accuracy_6: 0.6584 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.6597 - dense_3_accuracy_9: 0.34 - ETA: 0s - loss: 6.3285 - dense_3_loss: 1.7218 - dense_3_accuracy: 0.9784 - dense_3_accuracy_1: 0.9788 - dense_3_accuracy_2: 0.6976 - dense_3_accuracy_3: 0.3354 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9772 - dense_3_accuracy_6: 0.6663 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.6608 - dense_3_accuracy_9: 0.34 - ETA: 0s - loss: 6.3037 - dense_3_loss: 1.7179 - dense_3_accuracy: 0.9790 - dense_3_accuracy_1: 0.9797 - dense_3_accuracy_2: 0.7073 - dense_3_accuracy_3: 0.3391 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9753 - dense_3_accuracy_6: 0.6661 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.6620 - dense_3_accuracy_9: 0.34 - 1s 100us/step - loss: 6.2956 - dense_3_loss: 1.7182 - dense_3_accuracy: 0.9791 - dense_3_accuracy_1: 0.9800 - dense_3_accuracy_2: 0.7087 - dense_3_accuracy_3: 0.3434 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9748 - dense_3_accuracy_6: 0.6701 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.6604 - dense_3_accuracy_9: 0.3496 - val_loss: 6.2616 - val_dense_3_loss: 1.7112 - val_dense_3_accuracy: 0.9750 - val_dense_3_accuracy_1: 0.9770 - val_dense_3_accuracy_2: 0.6870 - val_dense_3_accuracy_3: 0.3540 - val_dense_3_accuracy_4: 1.0000 - val_dense_3_accuracy_5: 0.9780 - val_dense_3_accuracy_6: 0.6670 - val_dense_3_accuracy_7: 1.0000 - val_dense_3_accuracy_8: 0.6440 - val_dense_3_accuracy_9: 0.3560\n",
      "Epoch 35/40\n",
      "9000/9000 [==============================] - ETA: 0s - loss: 6.2162 - dense_3_loss: 1.7078 - dense_3_accuracy: 0.9814 - dense_3_accuracy_1: 0.9805 - dense_3_accuracy_2: 0.7212 - dense_3_accuracy_3: 0.3672 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9717 - dense_3_accuracy_6: 0.6699 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.6680 - dense_3_accuracy_9: 0.34 - ETA: 0s - loss: 6.1602 - dense_3_loss: 1.7060 - dense_3_accuracy: 0.9824 - dense_3_accuracy_1: 0.9822 - dense_3_accuracy_2: 0.7314 - dense_3_accuracy_3: 0.3630 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9741 - dense_3_accuracy_6: 0.6829 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.6638 - dense_3_accuracy_9: 0.35 - ETA: 0s - loss: 6.1356 - dense_3_loss: 1.6901 - dense_3_accuracy: 0.9801 - dense_3_accuracy_1: 0.9803 - dense_3_accuracy_2: 0.7345 - dense_3_accuracy_3: 0.3602 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9754 - dense_3_accuracy_6: 0.6857 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.6618 - dense_3_accuracy_9: 0.36 - ETA: 0s - loss: 6.1294 - dense_3_loss: 1.6897 - dense_3_accuracy: 0.9794 - dense_3_accuracy_1: 0.9796 - dense_3_accuracy_2: 0.7339 - dense_3_accuracy_3: 0.3661 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9756 - dense_3_accuracy_6: 0.6825 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.6647 - dense_3_accuracy_9: 0.36 - 1s 99us/step - loss: 6.1239 - dense_3_loss: 1.6785 - dense_3_accuracy: 0.9790 - dense_3_accuracy_1: 0.9797 - dense_3_accuracy_2: 0.7352 - dense_3_accuracy_3: 0.3677 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9742 - dense_3_accuracy_6: 0.6801 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.6658 - dense_3_accuracy_9: 0.3617 - val_loss: 6.1142 - val_dense_3_loss: 1.6333 - val_dense_3_accuracy: 0.9750 - val_dense_3_accuracy_1: 0.9740 - val_dense_3_accuracy_2: 0.7230 - val_dense_3_accuracy_3: 0.3230 - val_dense_3_accuracy_4: 1.0000 - val_dense_3_accuracy_5: 0.9790 - val_dense_3_accuracy_6: 0.6830 - val_dense_3_accuracy_7: 1.0000 - val_dense_3_accuracy_8: 0.6490 - val_dense_3_accuracy_9: 0.3920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/40\n",
      "9000/9000 [==============================] - ETA: 0s - loss: 6.1373 - dense_3_loss: 1.6582 - dense_3_accuracy: 0.9702 - dense_3_accuracy_1: 0.9712 - dense_3_accuracy_2: 0.7275 - dense_3_accuracy_3: 0.3525 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9736 - dense_3_accuracy_6: 0.6670 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.6675 - dense_3_accuracy_9: 0.36 - ETA: 0s - loss: 6.0694 - dense_3_loss: 1.6368 - dense_3_accuracy: 0.9717 - dense_3_accuracy_1: 0.9719 - dense_3_accuracy_2: 0.7358 - dense_3_accuracy_3: 0.3721 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9724 - dense_3_accuracy_6: 0.6736 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.6658 - dense_3_accuracy_9: 0.37 - ETA: 0s - loss: 5.9990 - dense_3_loss: 1.6381 - dense_3_accuracy: 0.9761 - dense_3_accuracy_1: 0.9764 - dense_3_accuracy_2: 0.7463 - dense_3_accuracy_3: 0.3770 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9730 - dense_3_accuracy_6: 0.6803 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.6681 - dense_3_accuracy_9: 0.37 - ETA: 0s - loss: 5.9871 - dense_3_loss: 1.6374 - dense_3_accuracy: 0.9767 - dense_3_accuracy_1: 0.9775 - dense_3_accuracy_2: 0.7500 - dense_3_accuracy_3: 0.3784 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9712 - dense_3_accuracy_6: 0.6815 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.6731 - dense_3_accuracy_9: 0.37 - 1s 99us/step - loss: 5.9766 - dense_3_loss: 1.6229 - dense_3_accuracy: 0.9777 - dense_3_accuracy_1: 0.9784 - dense_3_accuracy_2: 0.7513 - dense_3_accuracy_3: 0.3798 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9710 - dense_3_accuracy_6: 0.6794 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.6726 - dense_3_accuracy_9: 0.3791 - val_loss: 5.9124 - val_dense_3_loss: 1.6109 - val_dense_3_accuracy: 0.9750 - val_dense_3_accuracy_1: 0.9770 - val_dense_3_accuracy_2: 0.7320 - val_dense_3_accuracy_3: 0.3990 - val_dense_3_accuracy_4: 1.0000 - val_dense_3_accuracy_5: 0.9780 - val_dense_3_accuracy_6: 0.6820 - val_dense_3_accuracy_7: 1.0000 - val_dense_3_accuracy_8: 0.6770 - val_dense_3_accuracy_9: 0.3830\n",
      "Epoch 37/40\n",
      "9000/9000 [==============================] - ETA: 0s - loss: 5.9145 - dense_3_loss: 1.6317 - dense_3_accuracy: 0.9766 - dense_3_accuracy_1: 0.9761 - dense_3_accuracy_2: 0.7388 - dense_3_accuracy_3: 0.3984 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9712 - dense_3_accuracy_6: 0.6831 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.6890 - dense_3_accuracy_9: 0.37 - ETA: 0s - loss: 5.8630 - dense_3_loss: 1.6171 - dense_3_accuracy: 0.9785 - dense_3_accuracy_1: 0.9788 - dense_3_accuracy_2: 0.7571 - dense_3_accuracy_3: 0.4036 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9707 - dense_3_accuracy_6: 0.6836 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.6870 - dense_3_accuracy_9: 0.39 - ETA: 0s - loss: 5.8389 - dense_3_loss: 1.6086 - dense_3_accuracy: 0.9790 - dense_3_accuracy_1: 0.9798 - dense_3_accuracy_2: 0.7603 - dense_3_accuracy_3: 0.3970 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9722 - dense_3_accuracy_6: 0.6885 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.6898 - dense_3_accuracy_9: 0.39 - ETA: 0s - loss: 5.8174 - dense_3_loss: 1.6008 - dense_3_accuracy: 0.9801 - dense_3_accuracy_1: 0.9805 - dense_3_accuracy_2: 0.7631 - dense_3_accuracy_3: 0.3976 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9734 - dense_3_accuracy_6: 0.6919 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.6855 - dense_3_accuracy_9: 0.39 - 1s 101us/step - loss: 5.8144 - dense_3_loss: 1.6102 - dense_3_accuracy: 0.9796 - dense_3_accuracy_1: 0.9800 - dense_3_accuracy_2: 0.7638 - dense_3_accuracy_3: 0.3960 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9732 - dense_3_accuracy_6: 0.6909 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.6857 - dense_3_accuracy_9: 0.3937 - val_loss: 5.6964 - val_dense_3_loss: 1.5327 - val_dense_3_accuracy: 0.9760 - val_dense_3_accuracy_1: 0.9760 - val_dense_3_accuracy_2: 0.7600 - val_dense_3_accuracy_3: 0.3830 - val_dense_3_accuracy_4: 1.0000 - val_dense_3_accuracy_5: 0.9790 - val_dense_3_accuracy_6: 0.6840 - val_dense_3_accuracy_7: 1.0000 - val_dense_3_accuracy_8: 0.6710 - val_dense_3_accuracy_9: 0.4130\n",
      "Epoch 38/40\n",
      "9000/9000 [==============================] - ETA: 0s - loss: 5.7173 - dense_3_loss: 1.5491 - dense_3_accuracy: 0.9775 - dense_3_accuracy_1: 0.9780 - dense_3_accuracy_2: 0.7720 - dense_3_accuracy_3: 0.3931 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9673 - dense_3_accuracy_6: 0.6807 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.6934 - dense_3_accuracy_9: 0.41 - ETA: 0s - loss: 5.6918 - dense_3_loss: 1.5798 - dense_3_accuracy: 0.9785 - dense_3_accuracy_1: 0.9797 - dense_3_accuracy_2: 0.7749 - dense_3_accuracy_3: 0.4070 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9685 - dense_3_accuracy_6: 0.7043 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.6899 - dense_3_accuracy_9: 0.40 - ETA: 0s - loss: 5.6644 - dense_3_loss: 1.5719 - dense_3_accuracy: 0.9784 - dense_3_accuracy_1: 0.9800 - dense_3_accuracy_2: 0.7741 - dense_3_accuracy_3: 0.4154 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9704 - dense_3_accuracy_6: 0.7030 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.6963 - dense_3_accuracy_9: 0.40 - ETA: 0s - loss: 5.6373 - dense_3_loss: 1.5587 - dense_3_accuracy: 0.9792 - dense_3_accuracy_1: 0.9801 - dense_3_accuracy_2: 0.7802 - dense_3_accuracy_3: 0.4204 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9722 - dense_3_accuracy_6: 0.6998 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.6957 - dense_3_accuracy_9: 0.40 - 1s 101us/step - loss: 5.6148 - dense_3_loss: 1.5416 - dense_3_accuracy: 0.9800 - dense_3_accuracy_1: 0.9809 - dense_3_accuracy_2: 0.7809 - dense_3_accuracy_3: 0.4230 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9733 - dense_3_accuracy_6: 0.7011 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.6939 - dense_3_accuracy_9: 0.4140 - val_loss: 5.5633 - val_dense_3_loss: 1.5135 - val_dense_3_accuracy: 0.9790 - val_dense_3_accuracy_1: 0.9790 - val_dense_3_accuracy_2: 0.7840 - val_dense_3_accuracy_3: 0.4180 - val_dense_3_accuracy_4: 1.0000 - val_dense_3_accuracy_5: 0.9760 - val_dense_3_accuracy_6: 0.7000 - val_dense_3_accuracy_7: 1.0000 - val_dense_3_accuracy_8: 0.6980 - val_dense_3_accuracy_9: 0.4110\n",
      "Epoch 39/40\n",
      "9000/9000 [==============================] - ETA: 0s - loss: 5.5647 - dense_3_loss: 1.5265 - dense_3_accuracy: 0.9771 - dense_3_accuracy_1: 0.9771 - dense_3_accuracy_2: 0.7734 - dense_3_accuracy_3: 0.4331 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9771 - dense_3_accuracy_6: 0.6929 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.7070 - dense_3_accuracy_9: 0.41 - ETA: 0s - loss: 5.5054 - dense_3_loss: 1.5138 - dense_3_accuracy: 0.9788 - dense_3_accuracy_1: 0.9792 - dense_3_accuracy_2: 0.7795 - dense_3_accuracy_3: 0.4312 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9741 - dense_3_accuracy_6: 0.7078 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.7036 - dense_3_accuracy_9: 0.42 - ETA: 0s - loss: 5.4868 - dense_3_loss: 1.5076 - dense_3_accuracy: 0.9787 - dense_3_accuracy_1: 0.9798 - dense_3_accuracy_2: 0.7881 - dense_3_accuracy_3: 0.4326 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9749 - dense_3_accuracy_6: 0.7065 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.7018 - dense_3_accuracy_9: 0.42 - ETA: 0s - loss: 5.4439 - dense_3_loss: 1.5022 - dense_3_accuracy: 0.9794 - dense_3_accuracy_1: 0.9806 - dense_3_accuracy_2: 0.7936 - dense_3_accuracy_3: 0.4406 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9742 - dense_3_accuracy_6: 0.7113 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.7013 - dense_3_accuracy_9: 0.42 - 1s 101us/step - loss: 5.4425 - dense_3_loss: 1.5078 - dense_3_accuracy: 0.9798 - dense_3_accuracy_1: 0.9807 - dense_3_accuracy_2: 0.7926 - dense_3_accuracy_3: 0.4411 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9739 - dense_3_accuracy_6: 0.7121 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.7013 - dense_3_accuracy_9: 0.4283 - val_loss: 5.3620 - val_dense_3_loss: 1.4590 - val_dense_3_accuracy: 0.9770 - val_dense_3_accuracy_1: 0.9740 - val_dense_3_accuracy_2: 0.7830 - val_dense_3_accuracy_3: 0.4420 - val_dense_3_accuracy_4: 1.0000 - val_dense_3_accuracy_5: 0.9790 - val_dense_3_accuracy_6: 0.7100 - val_dense_3_accuracy_7: 1.0000 - val_dense_3_accuracy_8: 0.7030 - val_dense_3_accuracy_9: 0.4360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/40\n",
      "9000/9000 [==============================] - ETA: 0s - loss: 5.3612 - dense_3_loss: 1.4796 - dense_3_accuracy: 0.9741 - dense_3_accuracy_1: 0.9736 - dense_3_accuracy_2: 0.8042 - dense_3_accuracy_3: 0.4429 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9766 - dense_3_accuracy_6: 0.7188 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.7051 - dense_3_accuracy_9: 0.43 - ETA: 0s - loss: 5.3257 - dense_3_loss: 1.4758 - dense_3_accuracy: 0.9758 - dense_3_accuracy_1: 0.9771 - dense_3_accuracy_2: 0.8042 - dense_3_accuracy_3: 0.4500 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9778 - dense_3_accuracy_6: 0.7161 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.6968 - dense_3_accuracy_9: 0.44 - ETA: 0s - loss: 5.3091 - dense_3_loss: 1.4732 - dense_3_accuracy: 0.9785 - dense_3_accuracy_1: 0.9792 - dense_3_accuracy_2: 0.8084 - dense_3_accuracy_3: 0.4478 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9744 - dense_3_accuracy_6: 0.7197 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.6981 - dense_3_accuracy_9: 0.43 - ETA: 0s - loss: 5.3057 - dense_3_loss: 1.4775 - dense_3_accuracy: 0.9796 - dense_3_accuracy_1: 0.9806 - dense_3_accuracy_2: 0.8035 - dense_3_accuracy_3: 0.4543 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9725 - dense_3_accuracy_6: 0.7170 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.7070 - dense_3_accuracy_9: 0.43 - 1s 102us/step - loss: 5.3043 - dense_3_loss: 1.4833 - dense_3_accuracy: 0.9797 - dense_3_accuracy_1: 0.9806 - dense_3_accuracy_2: 0.8028 - dense_3_accuracy_3: 0.4556 - dense_3_accuracy_4: 1.0000 - dense_3_accuracy_5: 0.9727 - dense_3_accuracy_6: 0.7178 - dense_3_accuracy_7: 1.0000 - dense_3_accuracy_8: 0.7073 - dense_3_accuracy_9: 0.4383 - val_loss: 5.5178 - val_dense_3_loss: 1.6412 - val_dense_3_accuracy: 0.9770 - val_dense_3_accuracy_1: 0.9740 - val_dense_3_accuracy_2: 0.8020 - val_dense_3_accuracy_3: 0.4500 - val_dense_3_accuracy_4: 1.0000 - val_dense_3_accuracy_5: 0.9760 - val_dense_3_accuracy_6: 0.7020 - val_dense_3_accuracy_7: 1.0000 - val_dense_3_accuracy_8: 0.6870 - val_dense_3_accuracy_9: 0.3830\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history=model.fit([Xoh, s0, c0], outputs, epochs=40, batch_size=2048,validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for model1\n",
    "history=model.fit([Xoh], outputs, epochs=40, batch_size=2048,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for model2\n",
    "history2=model2.fit([Xoh, pred0], outputs, epochs=120, batch_size=2048,validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While training you can see the loss as well as the accuracy on each of the 10 positions of the output. The table below gives you an example of what the accuracies could be if the batch had 2 examples: \n",
    "\n",
    "<img src=\"images/table.png\" style=\"width:700;height:200px;\"> <br>\n",
    "<caption><center>Thus, `dense_2_acc_8: 0.89` means that you are predicting the 7th character of the output correctly 89% of the time in the current batch of data. </center></caption>\n",
    "\n",
    "\n",
    "We have run this model for longer, and saved the weights. Run the next cell to load our weights. (By training a model for several minutes, you should be able to obtain a model of similar accuracy, but loading our model will save you time.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将模型保存到文件 my_model.h5\n",
    "model2.save('models/xrh_model2.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#载入模型 \n",
    "model2.load_weights('models/xrh_model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#查看 训练完成的 模型 里面的参数\n",
    "all_configs=model2.get_config()\n",
    "all_configs['input_layers']\n",
    "all_configs['output_layers']\n",
    "all_configs['layers'][11]\n",
    "weights = model2.layers[11].get_weights() # Getting params\n",
    "# model.layers[i].set_weights(weights) # Setting par\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    " \n",
    " \n",
    "a=np.array(\n",
    "[[1,2,3,4,5],\n",
    "[1,2,3,4,5],\n",
    "[1,2,3,4,5]]    \n",
    ")\n",
    "# a=np.array([[2.29982214e-10,1.05035841e-03,1.04089566e-04,9.98845458e-01,\n",
    "#   5.10124494e-08,5.03688757e-10,2.52189380e-10,1.18713073e-09,\n",
    "#   2.30988277e-08,2.21948682e-08,1.48340121e-07]])\n",
    "\n",
    "\n",
    "\n",
    "input = tf.constant(a)\n",
    "k = 3\n",
    "output = tf.nn.top_k(input, k).indices\n",
    "\n",
    "\n",
    "# one_hot=one_hot_tensor(output,11)\n",
    "# one_hot=one_hot[0]\n",
    "# # one_hot[0].shape\n",
    "# one_hot=K.reshape(one_hot,(1,one_hot.shape[0],one_hot.shape[1]))\n",
    "# # one_hot.shape\n",
    "# one_hot_permute=K.permute_dimensions(one_hot,(1,0,2))\n",
    "# one_hot_permute.shape\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(input))\n",
    "    print(sess.run(output))\n",
    "#     print(sess.run(one_hot))\n",
    "#     print(sess.run(one_hot_permute))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  tf.nn.top_k 输出每一行 的topk 我们希望能输出整个矩阵的 topk\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    " \n",
    "\n",
    "def all_top_k(input,k):\n",
    "\n",
    "    flatten=K.flatten(input)\n",
    "    global_top_k=tf.nn.top_k(flatten, k)\n",
    "    print('global topk values:',K.eval(global_top_k.values))\n",
    "    print('glaobal topk indices:',K.eval(global_top_k.indices))\n",
    "    indices=global_top_k.indices\n",
    "\n",
    "    indices_row= K.cast(tf.floor(indices/input.shape[-1]),dtype='int32') \n",
    "#     K.eval(indices_row)\n",
    "\n",
    "\n",
    "    indices_col=indices%input.shape[-1] # dtype='int32'\n",
    "    # indices_col=tf.mod(indices,a.shape[-1]) #  tensorflow 的数学运算 https://blog.csdn.net/zywvvd/article/details/78593618\n",
    "\n",
    "#     K.eval(indices_col)\n",
    "\n",
    "    indices=K.concatenate( [K.reshape(indices_row,(1,indices_row.shape[0])) , K.reshape(indices_col,(1,indices_col.shape[0]))] , axis=0)\n",
    "    indices=K.transpose(indices)\n",
    "    \n",
    "    return indices\n",
    " \n",
    "    \n",
    "a=np.array(\n",
    "[[1,2,3,4,5],\n",
    "[1,2,2,2,2],\n",
    "[1,3,3,3,6]]    \n",
    ")\n",
    "\n",
    "\n",
    "k = 3\n",
    "# result=K.eval(all_top_k(a,k))\n",
    "# result\n",
    "# result.shape\n",
    "\n",
    "\n",
    "decoder_result=[]\n",
    "# decoder_result=np.zeros((k,Ty))\n",
    "r0=np.array([3, 1, 2])\n",
    "r0=np.reshape(r0,(3,1))\n",
    "\n",
    "# decoder_result[:,0]=b\n",
    "decoder_result.append(r0)\n",
    "decoder_result\n",
    "\n",
    "r=np.array([[0, 1],\n",
    "             [2, 2],\n",
    "             [1, 1]])\n",
    "\n",
    "# r0=decoder_result[0]\n",
    "\n",
    "r_pre=decoder_result[0]\n",
    "print('r_pre:',r_pre)\n",
    "\n",
    "r1=K.cast(K.zeros((k,2)),dtype='int32')\n",
    "\n",
    "#TODO:  build a empty tensor: r1\n",
    "\n",
    "#TODO:  少在 tensor 和 numpy 之间的来回转换 可以提升速度？ 全部用tensor 进行计算 \n",
    "\n",
    "for i in range(k):\n",
    "    a=K.reshape(r_pre[r[i][0]],(1,r_pre.shape[1])) \n",
    "    b=K.reshape(r[i][1],(1,1))\n",
    "    \n",
    "    c=K.concatenate( [a,b]  ,axis=1 )\n",
    "    print(c)\n",
    "#     r1[i,:].assign( K.concatenate( [a,b]  ,axis=0 ) ) #ValueError: Sliced assignment is only supported for variables\n",
    "# TODO: 两个 tensor 之间的切片 赋值   \n",
    "    \n",
    "    \n",
    "decoder_result.append(r1)\n",
    "decoder_result\n",
    "\n",
    "# r_pre=decoder_result[1]\n",
    "# print('r_pre:',r_pre)\n",
    "\n",
    "# r=np.array([[0, 4],\n",
    "#              [0, 2],\n",
    "#              [0, 3]])\n",
    "\n",
    "# r2=np.zeros((k,3))\n",
    "\n",
    "# for i in range(k):\n",
    "#     r2[i,:]=np.concatenate( ( r_pre[r[i][0]],[r[i][1]] ),axis=0 )\n",
    "\n",
    "\n",
    "# decoder_result.append(r2)\n",
    "# decoder_result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##--part1--: 使用 numpy 复现 tf.nn.topk  ## \n",
    "\n",
    "# k=3\n",
    "# arr = np.array([1, 98, 2, 99, 100])\n",
    "# idx=arr.argsort()[::-1][0:k]\n",
    "# idx\n",
    "# arr[idx]# 最大的三个元素 (已排序)\n",
    "\n",
    "# idx=np.argpartition(arr, k)[0:k]\n",
    "# arr[idx]#最小的 三个元素\n",
    "\n",
    "# idx = np.argpartition(arr, -k)[-k:]\n",
    "# idx\n",
    "# arr[idx]#最大的 三个元素 (未排序)\n",
    "\n",
    "# a=np.array(\n",
    "# [[1,2,3,4,5],\n",
    "# [1,2,8,2,2],\n",
    "# [9,3,3,3,6]]    \n",
    "# )\n",
    "# np.argpartition(a, -k)\n",
    "# idx = np.argpartition(a, -k)[ :,-k:]\n",
    "# idx\n",
    "\n",
    "\n",
    "def topk_array(matrix, k, axis=1):\n",
    "    \"\"\"\n",
    "    perform topK based on np.argsort\n",
    "    :param matrix: to be sorted\n",
    "    :param K: select and sort the top K items\n",
    "    :param axis: dimension to be sorted.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    full_sort = np.argsort(matrix, axis=axis)\n",
    "    return full_sort[ :,-k:]\n",
    "\n",
    "def partition_topk_array(matrix, K, axis=1):\n",
    "    \"\"\"\n",
    "    perform topK based on np.argpartition\n",
    "    :param matrix: to be sorted\n",
    "    :param K: select and sort the top K items\n",
    "    :param axis: 0 or 1. dimension to be sorted.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    a_part = np.argpartition(matrix, -K, axis=axis)\n",
    "    if axis == 0:\n",
    "        row_index = np.arange(matrix.shape[1 - axis])\n",
    "        a_sec_argsort_K = np.argsort(matrix[a_part[-K:, :], row_index], axis=axis)\n",
    "        return a_part[-K:, :][a_sec_argsort_K, row_index]\n",
    "    else:\n",
    "        column_index = np.arange(matrix.shape[1 - axis])[:, None]\n",
    "#         print('column_index ',column_index)\n",
    "#         print('matrix[column_index, a_part[:, -K:]] ',matrix[column_index, a_part[:, -K:]]) #选取矩阵中的一组元素\n",
    "        a_sec_argsort_K = np.argsort(matrix[column_index, a_part[:, -K:]], axis=axis)\n",
    "#         print('a_sec_argsort_K ',a_sec_argsort_K)\n",
    "        return a_part[:, -K:][column_index, a_sec_argsort_K] # 乾坤大挪移，变换矩阵中的元素位置\n",
    "\n",
    "    \n",
    "\n",
    "# arr = np.array([[1, 98, 2, 99, 100]])\n",
    "\n",
    "a=np.array(\n",
    "[[1,2,3,4,5],\n",
    "[1,2,8,2,2],\n",
    "[9,3,3,3,6]]    \n",
    ")\n",
    "k=3\n",
    "# partition_topk_array(a, k, axis=1) \n",
    "\n",
    "# partition_topk_array(arr, k, axis=1)# 最大的 三个元素 (已排序)\n",
    "\n",
    "\n",
    "## --ref: \n",
    "# https://stackoverflow.com/questions/6910641/how-do-i-get-indices-of-n-maximum-values-in-a-numpy-array\n",
    "# https://stackoverflow.com/questions/41484104/how-numpy-partition-work\n",
    "# https://blog.csdn.net/SoftPoeter/article/details/86629329\n",
    "##--part1-- end --##\n",
    "\n",
    "##--part2--: arg_topK 输出 二维矩阵的 每一行 的topk，我们希望能输出整个矩阵的 topk\n",
    "\n",
    "def whole_topk_array(input,k):\n",
    "    \"\"\"\n",
    "    输出 input 中所有元素中的 k 个最大的元素的下标，但是这k个元素并不会按照大小排序\n",
    "    \"\"\"\n",
    "\n",
    "    flatten=input.flatten()\n",
    "#     print(flatten)\n",
    "    \n",
    "    global_top_k=np.argpartition(flatten, -k)[-k:]\n",
    "    \n",
    "    indices=global_top_k\n",
    "\n",
    "    indices_row= np.floor(indices/input.shape[-1])\n",
    "    \n",
    "\n",
    "    indices_col=indices%input.shape[-1] # dtype='int32'\n",
    "\n",
    "\n",
    "    indices=np.concatenate( [np.reshape(indices_row,(1,indices_row.shape[0])) , np.reshape(indices_col,(1,indices_col.shape[0]))] , axis=0)\n",
    "    indices=np.transpose(indices)\n",
    "    \n",
    "    return indices.astype(np.int32) # numpy 数据类型转换 ；查看数据类型： arr.dtype\n",
    "\n",
    "\n",
    "# whole_topk_array(a,k)\n",
    "\n",
    "##--part2-- end --##\n",
    "\n",
    "##--part3--: 使用 numpy 复现   tf.one_hot()\n",
    "\n",
    "# a.reshape(-1,3) # 固定3列 (-1)=多少行不知道，numpy自己算去吧\n",
    "# a.reshape(-1) #  flatten a \n",
    "\n",
    "def one_hot_array(targets, nb_classes):\n",
    "    res = np.eye(nb_classes)[np.array(targets).reshape(-1)]\n",
    "#     print(res.shape)\n",
    "    return res.reshape(list(targets.shape)+[nb_classes])\n",
    "\n",
    "# a.shape\n",
    "# b=one_hot_array(a,11)\n",
    "# b.shape\n",
    "\n",
    "##--ref: \n",
    "# https://stackoverflow.com/questions/38592324/one-hot-encoding-using-numpy\n",
    "##--part3-- end --##\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# By XRH in 2019.9.10\n",
    "# beamsearch 的两种实现方式，并比较时间开销  \n",
    "\n",
    "# by : https://stackoverflow.com/questions/48374905/how-can-i-use-argsort-in-keras\n",
    "def top_k(input, k):\n",
    "  # Can also use `.values` to return a sorted tensor\n",
    "  return tf.nn.top_k(input, k=k, sorted=True)\n",
    "\n",
    "\n",
    "def all_top_k(input,k):\n",
    "    \"\"\"\n",
    "     tf.nn.top_k 输出每一行 的topk 我们希望能输出整个矩阵的 topk\n",
    "    \"\"\"\n",
    "\n",
    "    flatten=K.flatten(input)\n",
    "    global_top_k=tf.nn.top_k(flatten, k)\n",
    "#     print('global topk values:',K.eval(global_top_k.values))\n",
    "#     print('glaobal topk indices:',K.eval(global_top_k.indices))\n",
    "    indices=global_top_k.indices\n",
    "\n",
    "    indices_row= K.cast(tf.floor(indices/input.shape[-1]),dtype='int32') \n",
    "#     K.eval(indices_row)\n",
    "\n",
    "\n",
    "    indices_col=indices%input.shape[-1] # dtype='int32'\n",
    "    # indices_col=tf.mod(indices,a.shape[-1]) #  tensorflow 的数学运算 https://blog.csdn.net/zywvvd/article/details/78593618\n",
    "\n",
    "#     K.eval(indices_col)\n",
    "\n",
    "    indices=K.concatenate( [K.reshape(indices_row,(1,indices_row.shape[0])) , K.reshape(indices_col,(1,indices_col.shape[0]))] , axis=0)\n",
    "    indices=K.transpose(indices)\n",
    "    \n",
    "    return indices\n",
    "\n",
    "\n",
    "def model2_onestep_decode(Tx, Ty,timestep, n_a, n_s, human_vocab_size, machine_vocab_size):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Tx -- length of the input sequence\n",
    "    timestep -- timestep of decoder \n",
    "    Ty -- length of the output sequence\n",
    "    n_a -- hidden state size of the Bi-LSTM\n",
    "    n_s -- hidden state size of the post-attention LSTM\n",
    "    human_vocab_size -- size of the python dictionary \"human_vocab\"\n",
    "    machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    X = Input(shape=(Tx, human_vocab_size)) # shape: (m,Tx,human_vocab_size)\n",
    "    s0 = Input(shape=(n_s,), name='s')  # shape of s:  (m, 64)\n",
    "    c0 = Input(shape=(n_s,), name='c')  # shape of c:  (m, 64)\n",
    "    \n",
    "    pred0=Input(shape=(1,len(machine_vocab)), name='pred')  # shape of pred (m ,1, 11)\n",
    "    \n",
    "    s=s0 # unmutable object: a new tensor is generated \n",
    "    c=c0\n",
    "    pred=pred0\n",
    "    \n",
    "    \n",
    "#     print('pred: after Input',pred)\n",
    "\n",
    "    \n",
    "    a, forward_h, forward_c, backward_h, backward_c= pre_activation_LSTM_cell(inputs=X) #  shape of a : (m,Tx, 2*n_a) \n",
    "    #TODO：这一步的推理是多余的，可以把 encoder 和 decoder 彻底解耦\n",
    "\n",
    "    \n",
    "    if timestep==0: # decoder 的第一个时间步\n",
    "\n",
    "            s = concatenate_s([forward_h, backward_h]) # shape of s:  (m, 64)\n",
    "            c = concatenate_c([forward_c, backward_c])\n",
    "            \n",
    "            context = one_step_attention(a, s) # shape of context :  (m, 1, 128)\n",
    "       \n",
    "            context=concatenate_context([context,pred])# shape of context: (m,128+11=139)\n",
    "\n",
    "            s, _, c = post_activation_LSTM_cell(inputs=context,initial_state=[s, c])\n",
    "\n",
    "            out = output_layer(s)   \n",
    "    else:\n",
    "            \n",
    "            context = one_step_attention(a, s) # shape of context :  (m, 1, 128)\n",
    "       \n",
    "            context=concatenate_context([context,pred])# shape of context: (m,128+11=139)\n",
    "\n",
    "            s, _, c = post_activation_LSTM_cell(inputs=context,initial_state=[s, c])\n",
    "\n",
    "            out = output_layer(s)    \n",
    "    \n",
    "    outputs=[s,c,out] # 输出 s c out 作为下一个时间步使用\n",
    "        \n",
    "          \n",
    "    model =  Model(inputs=[X, s0, c0 ,pred0], outputs=outputs) \n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def beamsearch(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size,k=3):\n",
    "    \"\"\"\n",
    "    @deprecated: too slow\n",
    "    cost time: 1min30s\n",
    "    \"\"\"\n",
    "    \n",
    "    s0 = np.zeros((k, n_s))\n",
    "    c0 = np.zeros((k, n_s))\n",
    "    pred0=np.zeros((k,1,len(machine_vocab)))\n",
    "    \n",
    "    s=s0\n",
    "    c=c0\n",
    "    pred=pred0\n",
    "    \n",
    "    decoder_result=[]\n",
    "    \n",
    "    for timestep in range(Ty):\n",
    "        \n",
    "        onestep_decode = model2_onestep_decode(Tx, Ty,timestep, n_a, n_s, len(human_vocab), len(machine_vocab))\n",
    "        s,c,out=onestep_decode.predict([source_oh, s, c,pred])\n",
    "        \n",
    "        \n",
    "        if timestep==0:\n",
    "            print('timestep :', timestep)\n",
    "\n",
    "#             print ('out:',out) # shape:(3, 11)\n",
    "\n",
    "            out_top_K=top_k(out, k).indices  #  shape:(3,3) \n",
    "           \n",
    "            top_K_indices=K.eval(out_top_K) # cost much time\n",
    "            \n",
    "            r0=top_K_indices[0]\n",
    "            \n",
    "            r0=np.reshape(r0,(k,1))\n",
    "            decoder_result=r0\n",
    "            \n",
    "            one_hot=one_hot_tensor(out_top_K,machine_vocab_size )\n",
    "#             print (K.eval(one_hot)) # tensor shape:(3,3,11)  \n",
    "            \n",
    "            one_hot=one_hot[0]\n",
    "#             print(K.eval(one_hot)) #  shape:(3, 11) for debug, get the value of tensor\n",
    "            one_hot=K.reshape(one_hot,(1,one_hot.shape[0],one_hot.shape[1])) #shape:(1,3, 11)\n",
    "            one_hot_permute=K.permute_dimensions(one_hot,(1,0,2)) #shape: (3,1,11)\n",
    "            \n",
    "            pred=K.eval(one_hot_permute) # tensor -> numpy array  \n",
    "#             print('pred shape:',pred.shape)\n",
    "        \n",
    "        else:\n",
    "            print('timestep :', timestep)\n",
    "#             print ('out:',out)\n",
    "            \n",
    "            out_top_K=all_top_k(out,k)\n",
    "            \n",
    "            r=K.eval(out_top_K)\n",
    "#             print('r:',r)\n",
    "            \n",
    "            r_pre=decoder_result\n",
    "    \n",
    "#             print('r_pre:',r_pre)\n",
    "\n",
    "            rt=np.zeros((k,timestep+1))\n",
    "\n",
    "            for i in range(k):\n",
    "\n",
    "                rt[i,:]=np.concatenate( ( r_pre[r[i][0]],[r[i][1]] ) , axis=0 )\n",
    "\n",
    "            decoder_result=rt\n",
    "            \n",
    "            \n",
    "            one_hot=one_hot_tensor(r[:,1],machine_vocab_size )\n",
    "#             print (K.eval(one_hot)) # shape:(3, 11)\n",
    "            \n",
    "            one_hot=K.reshape(one_hot,(1,one_hot.shape[0],one_hot.shape[1])) #shape:(1,3, 11)\n",
    "#             print('one_hot shape:',one_hot.shape)\n",
    "            one_hot_permute=K.permute_dimensions(one_hot,(1,0,2)) #shape: (3,1,11)\n",
    "            \n",
    "            pred=K.eval(one_hot_permute) # tensor -> numpy array  \n",
    "#             print('pred shape:',pred.shape)\n",
    "            \n",
    "            \n",
    "        print('decoder_result',decoder_result) \n",
    "    \n",
    "    return   decoder_result  \n",
    "        \n",
    "\n",
    "def beamsearch_v1(source_oh,Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size,k):\n",
    "    \"\"\"\n",
    "    np.array -> tensor 很自然 但是 tensor -> np.array 的方法： K.eval(tensor) 非常耗费时间；\n",
    "    beamsearch_v1 尝试尽量多用 numpy 库的函数，以减少 tensor 和 np.array 的转换的次数。\n",
    "    cost time: 6.13 s\n",
    "    \"\"\"\n",
    "    \n",
    "    s0 = np.zeros((k, n_s))\n",
    "    c0 = np.zeros((k, n_s))\n",
    "    pred0=np.zeros((k,1,len(machine_vocab)))\n",
    "    \n",
    "    s=s0\n",
    "    c=c0\n",
    "    pred=pred0\n",
    "    \n",
    "    decoder_result=[]\n",
    "    \n",
    "    for timestep in range(Ty):\n",
    "        \n",
    "        onestep_decode = model2_onestep_decode(Tx, Ty,timestep, n_a, n_s, len(human_vocab), len(machine_vocab))\n",
    "        s,c,out=onestep_decode.predict([source_oh, s, c,pred]) \n",
    "        #source_oh shape：(3, 30, 37) \n",
    "        #每次都对 3个相同的样本（k=3）进行 推理，但是每一个 样本对应的 pred 不同 ；\n",
    "        #这实现了beamsearch 中，每一个时间步都会根据上一步的 onestep_decoder 输出结果中 选择最好的k个, 输入 onestep_decoder \n",
    "        \n",
    "        if timestep==0:\n",
    "            print('timestep :', timestep)\n",
    "\n",
    "#             print ('out:',out) # shape:(3, 11)\n",
    "\n",
    "            out_top_K=partition_topk_array(out, k)  #  shape:(3,3) \n",
    "            print(out_top_K)\n",
    "           \n",
    "            top_K_indices=out_top_K \n",
    "            \n",
    "            r0=top_K_indices[0]\n",
    "            \n",
    "            r0=np.reshape(r0,(k,1))\n",
    "            decoder_result=r0\n",
    "            \n",
    "            one_hot=one_hot_array(out_top_K,machine_vocab_size )#  shape:(3,3,11)\n",
    "            \n",
    "            one_hot=one_hot[0]#  shape:(3, 11) \n",
    "            one_hot=np.reshape(one_hot,(1,one_hot.shape[0],one_hot.shape[1])) #shape:(1,3, 11)\n",
    "            \n",
    "            one_hot_permute=one_hot.transpose((1,0,2)) #shape: (3,1,11)\n",
    "            \n",
    "            pred=one_hot_permute   \n",
    "        \n",
    "        else:\n",
    "            print('timestep :', timestep)\n",
    "#             print ('out:',out)\n",
    "            \n",
    "            out_top_K=whole_topk_array(out,k)\n",
    "            \n",
    "            r=out_top_K\n",
    "            \n",
    "            r_pre=decoder_result\n",
    "    \n",
    "\n",
    "            rt=np.zeros((k,timestep+1))\n",
    "\n",
    "            for i in range(k):\n",
    "                \n",
    "\n",
    "                rt[i,:]=np.concatenate( ( r_pre[r[i][0]],[r[i][1]] ) , axis=0 )\n",
    "\n",
    "            decoder_result=rt\n",
    "            \n",
    "            \n",
    "            one_hot=one_hot_array(r[:,1],machine_vocab_size ) # shape:(3, 11)\n",
    "            \n",
    "            one_hot=np.reshape(one_hot,(1,one_hot.shape[0],one_hot.shape[1])) #shape:(1,3, 11)\n",
    "            one_hot_permute=one_hot.transpose((1,0,2)) #shape: (3,1,11)\n",
    "            \n",
    "            pred=one_hot_permute\n",
    "            \n",
    "            \n",
    "        print('decoder_result',decoder_result) \n",
    "    \n",
    "    return   decoder_result  \n",
    "        \n",
    "    \n",
    "example = \"3rd of March 2002\"\n",
    "source = np.array(string_to_int(example, Tx, human_vocab))\n",
    "source_oh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source)))\n",
    "\n",
    "k=3\n",
    "source_oh=source_oh.reshape(1,source_oh.shape[0],source_oh.shape[1])   \n",
    "# print(source_oh.shape) \n",
    "source_oh=np.repeat(source_oh, k, axis=0) \n",
    "print(source_oh.shape) #(3, 30, 37) m=3 一个样本 复制为三个输入模型进行推理\n",
    "\n",
    "  \n",
    "decoder_result=beamsearch_v1(source_oh,Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab),k) \n",
    "\n",
    "for prediction in decoder_result:\n",
    "    output = int_to_string(prediction, inv_machine_vocab)\n",
    "    print(\"source:\", example)\n",
    "    print(\"output:\", ''.join(output))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "#   By XRH in 2019.9.17\n",
    "#  对 encoder 和 decoder 进行解耦\n",
    "\n",
    "def model2_onestep_decoder_v2(Tx, Ty,timestep, n_a, n_s, human_vocab_size, machine_vocab_size):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Tx -- length of the input sequence\n",
    "    timestep -- timestep of decoder \n",
    "    Ty -- length of the output sequence\n",
    "    n_a -- hidden state size of the Bi-LSTM\n",
    "    n_s -- hidden state size of the post-attention LSTM\n",
    "    human_vocab_size -- size of the python dictionary \"human_vocab\"\n",
    "    machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    context0=Input(shape=(1,n_s), name='context')\n",
    "    \n",
    "    s0 = Input(shape=(n_s,), name='s')  # shape of s:  (m, 64)\n",
    "    c0 = Input(shape=(n_s,), name='c')  # shape of c:  (m, 64)\n",
    "    \n",
    "    pred0=Input(shape=(1,len(machine_vocab)), name='pred')  # shape of pred (m ,1, 11)\n",
    "    \n",
    "    context=context0\n",
    "    s=s0 # unmutable object: a new tensor is generated \n",
    "    c=c0\n",
    "    pred=pred0\n",
    "     \n",
    "#     print('pred: after Input',pred)\n",
    "\n",
    "    context=concatenate_context([context,pred])# shape of context: (m,128+11=139)\n",
    "\n",
    "    s, _, c = post_activation_LSTM_cell(inputs=context,initial_state=[s, c])\n",
    "\n",
    "    out = output_layer(s)    \n",
    "    \n",
    "    outputs=[s,c,out] # 输出 s c out 作为下一个时间步使用\n",
    "        \n",
    "          \n",
    "    model =  Model(inputs=[context0 ,s0, c0 ,pred0], outputs=outputs) \n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def model2_encoder(Tx, human_vocab_size):\n",
    "    \n",
    "    X = Input(shape=(Tx, human_vocab_size)) # shape: (m,Tx,human_vocab_size)\n",
    "    \n",
    "    a, forward_h, forward_c, backward_h, backward_c= pre_activation_LSTM_cell(inputs=X) #  shape of a : (m,Tx, 2*n_a) \n",
    " \n",
    "    s = concatenate_s([forward_h, backward_h]) # shape of s:  (m, 64+64)\n",
    "    c = concatenate_c([forward_c, backward_c])\n",
    "    \n",
    "    context = one_step_attention(a, s) # shape of context :  (m, 1, 128)\n",
    "    \n",
    "    outputs=[context,s,c]\n",
    "    \n",
    "    model =  Model(inputs=[X], outputs=outputs) \n",
    "    \n",
    "    return model\n",
    "    \n",
    "\n",
    "def beamsearch_v2(source_oh,Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size,k=3):\n",
    "    \"\"\"\n",
    "    np.array -> tensor 很自然 但是 tensor -> np.array 的方式： K.eval(tensor) 非常耗费时间；\n",
    "    beamsearch_v1 尝试尽量多用 numpy 的函数，以减少 tensor 和 np 的转换的次数。\n",
    "    cost time: 6.13 s\n",
    "    \"\"\"\n",
    "    \n",
    "    s0 = np.zeros((k, n_s))\n",
    "    c0 = np.zeros((k, n_s))\n",
    "    pred0=np.zeros((k,1,len(machine_vocab)))\n",
    "    \n",
    "    s=s0\n",
    "    c=c0\n",
    "    pred=pred0\n",
    "    \n",
    "    decoder_result=[]\n",
    "\n",
    "#--encoder 和 decoder 的解耦\n",
    "#bearm serach encoder：\n",
    "# M1: \n",
    "#     encoder_output = Model(inputs=model2.input,  \n",
    "#         outputs=[ model2.get_layer('decoder_output').get_output_at(1) ]) #TODO\n",
    "#     a,s,c= encoder_output.predict([source_oh, s0, c0,pred0])\n",
    "\n",
    "# M2:\n",
    "\n",
    "    encoder=model2_encoder(Tx, human_vocab_size)\n",
    "    context,s,c=encoder.predict([source_oh])\n",
    "\n",
    "#bearm serach decoder：\n",
    "\n",
    "    for timestep in range(Ty):\n",
    "        \n",
    "        onestep_decoder = model2_onestep_decoder_v2(Tx, Ty,timestep, n_a, n_s, len(human_vocab), len(machine_vocab))\n",
    "        s,c,out=onestep_decoder.predict([context, s, c,pred]) \n",
    "        #source_oh shape：(3, 30, 37) \n",
    "        #每次都对 3个相同的样本（k=3）进行 推理，但是每一个 样本对应的 pred 不同 ；\n",
    "        #这实现了beamsearch 中，每一个时间步都会根据上一步的 onestep_decoder 输出结果中 选择最好的k个, 输入 onestep_decoder \n",
    "        \n",
    "        if timestep==0:\n",
    "            print('timestep :', timestep)\n",
    "\n",
    "#             print ('out:',out) # shape:(3, 11) softmax 层输出的为 11 unit 的概率；输入的样本数量为3\n",
    "\n",
    "            out_top_K=partition_topk_array(out, k)  #  shape:(3,3) 从每个样本 的 11unit 中选出最大的k个 \n",
    "            print(out_top_K)\n",
    "           \n",
    "            top_K_indices=out_top_K \n",
    "            \n",
    "            r0=top_K_indices[0] #  shape:(1,3) 因为3个输入样本是一样的，取其中一个即可 \n",
    "            \n",
    "            r0=np.reshape(r0,(k,1)) # shape:(3,1)\n",
    "            decoder_result=r0\n",
    "            \n",
    "            one_hot=one_hot_array(out_top_K,machine_vocab_size )#  shape:(3,3,11)\n",
    "            # 把 out_top_K shape:(3,3) 最后一个维度 变为 one-hot 向量\n",
    "            \n",
    "            one_hot=one_hot[0]#  shape:(3, 11) ；只要取一个即可\n",
    "            one_hot=np.reshape(one_hot,(1,one_hot.shape[0],one_hot.shape[1])) #shape:(1,3, 11)\n",
    "            \n",
    "            one_hot_permute=one_hot.transpose((1,0,2)) #shape: (3,1,11) ；\n",
    "            #交换 第0维 和 第1维，相当于3个不同的 pred 同时输入下一个时间步的 onestep_decoder\n",
    "            pred=one_hot_permute   \n",
    "        \n",
    "        else:\n",
    "            print('timestep :', timestep)\n",
    "#             print ('out:',out) # shape:(3, 11)\n",
    "            \n",
    "            out_top_K=whole_topk_array(out,k)  #  shape:(3, 2) 找出 3*11 个元素中的k个最大的 元素的标号 \n",
    "            \n",
    "            r=out_top_K \n",
    "            print('r:',r) \n",
    "#             [[1 1]    \n",
    "#              [0 1]\n",
    "#              [2 1]] 元素的标号为 [2,1] ，代表 第3个输入的pred 所输出的11个uints中的第1个unit\n",
    "            \n",
    "            r_pre=decoder_result # shape:(k,timestep) 上一步 解码的结果 即是 这一步的输入 \n",
    "            # [[2]\n",
    "            #  [1]\n",
    "            #  [3]]\n",
    "    \n",
    "            rt=np.zeros((k,timestep+1)) #这一步 会在上一步 已有的解码序列的基础上 增加1个 解码位\n",
    "\n",
    "            for i in range(k):\n",
    "                \n",
    "                rt[i,:]=np.concatenate( ( r_pre[r[i][0]],[r[i][1]] ) , axis=0 )\n",
    "                # r[2][0]=2 说明是第二个输入的pred，前一步的解码情况为： r_pre[r[2][0]]=[2] ，\n",
    "                #再连接上这一步的解码位  r[2][1]=1 得到 解码序列：[2,1]\n",
    "                # 一共k 个解码序列 组成 rt\n",
    "                \n",
    "            decoder_result=rt.astype(np.int32) \n",
    "#               rt:\n",
    "#              [[1. 1.] \n",
    "#              [2. 1.]\n",
    "#              [3. 1.]]\n",
    "\n",
    "#              decoder_result:\n",
    "#              [[1 1] \n",
    "#              [2 1]\n",
    "#              [3 1]]\n",
    "            \n",
    "            \n",
    "            one_hot=one_hot_array(decoder_result[:,-1],machine_vocab_size ) # shape:(3, 11)\n",
    "#             print(one_hot.shape)\n",
    "            \n",
    "            one_hot=np.reshape(one_hot,(1,one_hot.shape[0],one_hot.shape[1])) #shape:(1,3, 11)\n",
    "            one_hot_permute=one_hot.transpose((1,0,2)) #shape: (3,1,11)\n",
    "            \n",
    "            pred=one_hot_permute\n",
    "            \n",
    "        print('decoder_result',decoder_result) \n",
    "    \n",
    "    return   decoder_result  \n",
    "        \n",
    "    \n",
    "example = \"3rd of March 2002\"\n",
    "source = np.array(string_to_int(example, Tx, human_vocab))\n",
    "source_oh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source)))\n",
    "\n",
    "k=3\n",
    "source_oh=source_oh.reshape(1,source_oh.shape[0],source_oh.shape[1])   \n",
    "# print(source_oh.shape) \n",
    "source_oh=np.repeat(source_oh, k, axis=0) \n",
    "# print(source_oh.shape) #(3, 30, 37) m=3 一个样本 复制为三个输入模型进行推理\n",
    "\n",
    "  \n",
    "decoder_result=beamsearch_v2(source_oh,Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab),k) \n",
    "\n",
    "for prediction in decoder_result:\n",
    "    output = int_to_string(prediction, inv_machine_vocab)\n",
    "    print(\"source:\", example)\n",
    "    print(\"output:\", ''.join(output))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw pictures to show the training process and the accuracy in the verification set and training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#by model \n",
    "\n",
    "\n",
    "#---对 decoder 的10个时间步的 准确率求均值 --#\n",
    "#--start--#\n",
    "# Ty = 10\n",
    "Epoch_num=(history.history['dense_3_acc'])\n",
    "\n",
    "acc0=np.array(history.history['dense_3_acc'])\n",
    "acc0=acc0.reshape(40,1)\n",
    "\n",
    "acc=acc0\n",
    "\n",
    "for i in range(Ty):\n",
    "    if i != 0:\n",
    "        \n",
    "        acc_t=np.array(history.history['dense_3_acc_'+str(i)])\n",
    "        acc_t=acc_t.reshape(40,1)\n",
    "        acc=np.concatenate([acc,acc_t],axis=1)\n",
    "  \n",
    "\n",
    "acc=np.mean(acc,axis=1)\n",
    "print('acc.shape:',acc.shape)   \n",
    "\n",
    "val_acc0=np.array(history.history['val_dense_3_acc'])\n",
    "val_acc0=val_acc0.reshape(40,1)\n",
    "\n",
    "val_acc=val_acc0\n",
    "\n",
    "for i in range(Ty):\n",
    "    if i != 0:\n",
    "        \n",
    "        val_acc_t=np.array(history.history['val_dense_3_acc_'+str(i)])\n",
    "        val_acc_t=val_acc_t.reshape(40,1)\n",
    "        val_acc=np.concatenate([val_acc,val_acc_t],axis=1)  \n",
    "\n",
    "val_acc=np.mean(val_acc,axis=1)\n",
    "print(val_acc.shape)  \n",
    "#--- end --#\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.figure( figsize=(8,4), dpi=100 )\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#by model2\n",
    "#---对 decoder 的10个时间步的 准确率求均值 --#\n",
    "#--start--#\n",
    "# Ty = 10\n",
    "Epoch_num=len(history2.history['decoder_output_acc'])\n",
    "\n",
    "acc0=np.array(history2.history['decoder_output_acc'])\n",
    "acc0=acc0.reshape(Epoch_num,1)\n",
    "\n",
    "acc=acc0\n",
    "\n",
    "for i in range(Ty):\n",
    "    if i != 0:\n",
    "        \n",
    "        acc_t=np.array(history2.history['decoder_output_acc_'+str(i)])\n",
    "        acc_t=acc_t.reshape(Epoch_num,1)\n",
    "        acc=np.concatenate([acc,acc_t],axis=1)\n",
    "  \n",
    "\n",
    "acc=np.mean(acc,axis=1)\n",
    "print('acc.shape:',acc.shape)   \n",
    "\n",
    "val_acc0=np.array(history2.history['val_decoder_output_acc'])\n",
    "val_acc0=val_acc0.reshape(Epoch_num,1)\n",
    "\n",
    "val_acc=val_acc0\n",
    "\n",
    "for i in range(Ty):\n",
    "    if i != 0:\n",
    "        \n",
    "        val_acc_t=np.array(history2.history['val_decoder_output_acc_'+str(i)])\n",
    "        val_acc_t=val_acc_t.reshape(Epoch_num,1)\n",
    "        val_acc=np.concatenate([val_acc,val_acc_t],axis=1)  \n",
    "\n",
    "val_acc=np.mean(val_acc,axis=1)\n",
    "print(val_acc.shape)  \n",
    "#--- end --#\n",
    "\n",
    "loss = history2.history['loss']\n",
    "val_loss = history2.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.figure( figsize=(25,15), dpi=200 )\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now see the results on new examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "example = \"3rd of March 2002\"\n",
    "source = np.array(string_to_int(example, Tx, human_vocab))\n",
    "\n",
    "\n",
    "# model input: [Xoh, s0, c0]\n",
    "\n",
    "print(\"Xoh.shape:\", Xoh.shape)\n",
    "print(\"source.shape:\", source.shape)\n",
    "\n",
    "source_oh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source)))\n",
    "print(\"source_oh shape:\", source_oh.shape)\n",
    "source_oh=source_oh.reshape(1,source_oh.shape[0],source_oh.shape[1])\n",
    "print(\"source_oh shape after reshape:\", source_oh.shape)\n",
    "\n",
    "prediction = model2.predict([source_oh, s0, c0,pred00])\n",
    "# prediction = model.predict([source_oh, s0, c0])\n",
    "\n",
    "prediction=np.array(prediction)\n",
    "print('prediction.shape:',prediction.shape)\n",
    "\n",
    "prediction=prediction.swapaxes(0,1)\n",
    "\n",
    "\n",
    "print(\"Yoh.shape:\", Yoh.shape)\n",
    "print(\"prediction.shape:\", prediction.shape)\n",
    "prediction = np.argmax(prediction[0], axis = -1)\n",
    "prediction\n",
    "output = int_to_string(prediction, inv_machine_vocab)\n",
    "print(\"source:\", example)\n",
    "print(\"output:\", ''.join(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#输出 模型 中间层的计算结果\n",
    "# M1  \n",
    "# get_layer12_output_timestep_9 = K.function([model2.layers[0].input],\n",
    "#                                   [model2.layers[12].get_output_at(9)])\n",
    "# layer_output = get_layer12_output_timestep_9([source_oh, s0, c0,pred0])[0] #TODO exisits error \n",
    "\n",
    "# M2\n",
    "\n",
    "\n",
    "example = \"3rd of March 2002\"\n",
    "source = np.array(string_to_int(example, Tx, human_vocab))\n",
    "\n",
    "print(\"Xoh.shape:\", Xoh.shape)\n",
    "print(\"source.shape:\", source.shape)\n",
    "\n",
    "source_oh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source)))\n",
    "print(\"source_oh shape:\", source_oh.shape)\n",
    "source_oh=source_oh.reshape(1,source_oh.shape[0],source_oh.shape[1])\n",
    "print(\"source_oh shape after reshape:\", source_oh.shape)\n",
    "\n",
    "\n",
    "layer_timestep_0_1 = Model(inputs=model2.input,\n",
    "                                     outputs=[model2.get_layer('encoder_lstm').get_output_at(0)])\n",
    "#TODO: error : Output tensors to a Model must be the output of a Keras `Layer`\n",
    "#如何通过 get_output_at 拿到lstm layer 的输出\n",
    "\n",
    "\n",
    "layer_timestep_0_1 = layer_timestep_0_1.predict([source_oh, s0, c0,pred0]) \n",
    "# dense3_output_timestep_9.shape\n",
    "layer_timestep_0_1.shape\n",
    "\n",
    "\n",
    "model2.get_layer('encoder_lstm')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - BLEU score\n",
    "\n",
    "In this last part, you are going to implement the BLEU score to assess the effectiveness "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bleu import compute_bleu\n",
    "reference = [['this', 'is', 'small', 'test']]\n",
    "candidate = ['this', 'is', 'a', 'test']\n",
    "\n",
    "score = compute_bleu(reference, candidate)\n",
    "print (score)\n",
    "\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "print('Cumulative 1-gram: %f' % sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)))\n",
    "print('Cumulative 2-gram: %f' % sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0)))\n",
    "print('Cumulative 3-gram: %f' % sentence_bleu(reference, candidate, weights=(0.33, 0.33, 0.33, 0)))\n",
    "print('Cumulative 4-gram: %f' % sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bleu import compute_bleu,_get_ngrams\n",
    "\n",
    "\n",
    "# EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\n",
    "# print('shape Xoh:',np.shape(Xoh))\n",
    "\n",
    "One_EXAMPLES = ['3 May 1979']\n",
    "EXAMPLES = ['3 May 1979', '5 Apr 09', '20th February 2016', 'Wed 10 Jul 2007']\n",
    "GROUND_TRUTH = ['1979-05-03', '2009-04-05', '2016-02-20', '2007-07-10']\n",
    "\n",
    "\n",
    "for index,example in enumerate(EXAMPLES):\n",
    "    \n",
    "    source = string_to_int(example, Tx, human_vocab)\n",
    "#     print (source)\n",
    "    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source)))#.swapaxes(0,1)\n",
    "#     print(np.shape(source))\n",
    "    \n",
    "    prediction = model.predict([np.reshape(source,[1,np.shape(Xoh)[1],-1]), s0, c0])\n",
    "#     print(np.shape(prediction))\n",
    "    prediction = np.argmax(prediction, axis = -1) #作用于 最后一维的特征\n",
    "#     print (np.shape(prediction))\n",
    "#     print (prediction)\n",
    "    output = [inv_machine_vocab[int(i)] for i in prediction]\n",
    "    output=''.join(output)\n",
    "    print(\"source:\", example)\n",
    "    print(\"output:\", output)\n",
    "    \n",
    "    target=GROUND_TRUTH[index]\n",
    "    print(\"target:\", target)\n",
    "    \n",
    "    \n",
    "    print(\"BLEU score: \", compute_bleu([target.split('-')], output.split('-'))[0])\n",
    "    \n",
    "#     print(\"BLEU score: \", compute_bleu([[ch for ch in target]],[ch for ch in output] )[0])\n",
    "    \n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also change these examples to test with your own examples. The next part will give you a better sense on what the attention mechanism is doing--i.e., what part of the input the network is paying attention to when generating a particular output character. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Visualizing Attention (Optional / Ungraded)\n",
    "\n",
    "Since the problem has a fixed output length of 10, it is also possible to carry out this task using 10 different softmax units to generate the 10 characters of the output. But one advantage of the attention model is that each part of the output (say the month) knows it needs to depend only on a small part of the input (the characters in the input giving the month). We can  visualize what part of the output is looking at what part of the input.\n",
    "\n",
    "Consider the task of translating \"Saturday 9 May 2018\" to \"2018-05-09\". If we visualize the computed $\\alpha^{\\langle t, t' \\rangle}$ we get this: \n",
    "\n",
    "<img src=\"images/date_attention.png\" style=\"width:600;height:300px;\"> <br>\n",
    "<caption><center> **Figure 8**: Full Attention Map</center></caption>\n",
    "\n",
    "Notice how the output ignores the \"Saturday\" portion of the input. None of the output timesteps are paying much attention to that portion of the input. We see also that 9 has been translated as 09 and May has been correctly translated into 05, with the output paying attention to the parts of the input it needs to to make the translation. The year mostly requires it to pay attention to the input's \"18\" in order to generate \"2018.\" \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Getting the activations from the network\n",
    "\n",
    "Lets now visualize the attention values in your network. We'll propagate an example through the network, then visualize the values of $\\alpha^{\\langle t, t' \\rangle}$. \n",
    "\n",
    "To figure out where the attention values are located, let's start by printing a summary of the model ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Navigate through the output of `model.summary()` above. You can see that the layer named `attention_weights` outputs the `alphas` of shape (m, 30, 1) before `dot_2` computes the context vector for every time step $t = 0, \\ldots, T_y-1$. Lets get the activations from this layer.\n",
    "\n",
    "The function `attention_map()` pulls out the attention values from your model and plots them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "attention_map = plot_attention_map(model, human_vocab, inv_machine_vocab, \"Tuesday April 08 1993\", num = 6, n_s = 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the generated plot you can observe the values of the attention weights for each character of the predicted output. Examine this plot and check that where the network is paying attention makes sense to you.\n",
    "\n",
    "In the date translation application, you will observe that most of the time attention helps predict the year, and hasn't much impact on predicting the day/month."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations!\n",
    "\n",
    "\n",
    "You have come to the end of this assignment \n",
    "\n",
    "<font color='blue'> **Here's what you should remember from this notebook**:\n",
    "\n",
    "- Machine translation models can be used to map from one sequence to another. They are useful not just for translating human languages (like French->English) but also for tasks like date format translation. \n",
    "- An attention mechanism allows a network to focus on the most relevant parts of the input when producing a specific part of the output. \n",
    "- A network using an attention mechanism can translate from inputs of length $T_x$ to outputs of length $T_y$, where $T_x$ and $T_y$ can be different. \n",
    "- You can visualize attention weights $\\alpha^{\\langle t,t' \\rangle}$ to see what the network is paying attention to while generating each output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on finishing this assignment! You are now able to implement an attention model and use it to learn complex mappings from one sequence to another. "
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "n16CQ",
   "launcher_item_id": "npjGi"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
